\chapter{Learning Dictionaries for Multi-Channel Signals}

\section{Introduction}
When using a multi-layer dictionary model, the coefficients corresponding to a dictionary from one layer become the "signal" for the subsequent layer.  The number of channels for this "signal" is the number of dictionary filters from the previous layer.  Much of the literature on learning convolutional dictionaries is tailored to applications with signals that only have a small number of channels.  This chapter presents a novel method for learning convolutional dictionaries from and for multi-channel signals. 
\section{Dictionary Types}
There are many ways to construct a convolutional sparse representation of a multi-channel signal, but broadly the distinctions reduce down to if and how signal channels share dictionaries and coefficients, and if and how those non-shared entities interact across channels.

It is common in many applications for dictionary models to share dictionaries across channels, which requires the use multi-channel coefficients. If such models were used in a multi-layer dictionary model, the tensor rank would increase with each subsequent layer.

For this work, I focus instead on the multi-channel dictionary with shared coefficients. This structure matches that of convolutional neural networks, and the number of channels for a subsequent dictionary is the number of filters for the dictionary from the previous layer.
\section{Pursuit and Sparse Coding}
The dictionary model decomposes the signal $\vs_i$ into a dictionary $\mD$ (which generalizes to other signals) and the coefficients $\vx_i$ (which are specific to the signal $\vs_i$:

\begin{equation}
\vs_i \approx \mD\vx_i
\end{equation}

(Here the subscript $i$ specifies a particular signal and its corresponding coefficients.) A pursuit algorithm finds the coefficients $\vx_i$ corresponding to a particular signal $\vs_i$ for known dictionary $\mD$. If the number of dictionary atoms (columns) is larger than the dimension of the signal, then the number of unknowns is larger than the number of equations, and many solutions for $\vx_i$ represent $\vs_i$ equally well (at least in an L$2$ sense). Researchers and practitioners commonly either impose a sparsity constraint on the coefficients or ad a coefficient L$1$ penalty to the objective function, which removes the ambiguity from the problem construction. When such a penalty or constraint is added, pursuit is sometimes called sparse coding. With the added coefficient L$1$ penalty, the pursuit optimization problem looks like this:

\begin{equation}
\vx_i = \arg\min_{\vx} \frac{1}{2}\|\vs_i - \mD\vx\|_2^2 + \lambda \|\vx\|_1
\end{equation}

where $\lambda$ is a hyperparameter greater than zero controlling how much the L$1$ norm of the coefficients is penalized. Researchers have proposed many ways to solve this problem. If the dictionary is convolutional and the number of channels is low, a standard approach is to use the Alternating direction Method of Multipliers (ADMM) algorithm.

\section{ADMM}
ADMM is a convex-optimization algorithm used to solve the optimization problem:
\begin{equation}
\mA\vx + \mB\vy + \vc = 0 
\end{equation}

where $f$ and $g$ are convex functions. (I will address how to put the sparse coding problem in this form in the next section.)

The ADMM algorithm makes use of the augmented Lagrangian, a particular expression that has a saddle point at the solution to the constrained optimization problem:

\begin{equation}
\operatorname{L}_{\rho}(\vx,\vy,\vu) = f(\vx) + g(\vy) + \vu^H(\mA\vx + \mB\vy + \vc) + \frac{\rho}{2}\|\mA\vx + \mB\vy + \vc\|_2^2
\end{equation}

where $\rho$ is a hyperparameter greater than zero and $\vu$ is the dual variable for the constraints.

At the saddle-point solution, the augmented Lagrangian is at a minimum in respect to $\vx$ and $\vy$, but at a maximum in respect to $\vu$.

The ADMM algorithm is an iterative search for the saddle point of the augmented Lagrangian. Each iteration consists of a primal update for $\vx$, a primal update for $\vy$, and a dual update for $\vu$:

\begin{equation}
\vx^{(k + 1)} = \arg\min_{\vx} \operatorname{L}_{\rho}(\vx,\vy^{(k)},\vu^{(k)}
\end{equation}

\begin{equation}
\vy^{(k + 1)} = \arg\min_{\vy} \operatorname{L}_{\rho}(\vx^{(k + 1)},\vy,\vu^{(k)}
\end{equation}

\begin{equation}
\vu^{(k + 1)} = \vu^{(k)} + \rho(\mA\vx^{(k + 1)} + \mB\vy^{(k + 1)} + \vc)
\end{equation}

The primal updates serve to move towards the minimum of the augmented lagrangian in respect to $\vx$ and $\vy$ with $\vu$ fixed, and the dual update fixes $\vx$ and $\vy$, and performs gradient ascent on $\vu$ with stepsize $\rho$. Under very mild assumptions, this process converges to the saddle point of the augmented lagrangian, which matches the solution to the constrained optimization problem.

There are two common variations of the ADMM algorithm that it is important to be aware of.  The first is the scaled form, which comes from completing the square for the augmented lagrangian function:

\begin{equation}
\operatorname{L}_{\rho}(\vx,\vy,\vu) = f(\vx) + g(\vy) + \frac{\rho}{2}\|\mA\vx + \mB\vy + \vc + \frac{\vu}{\rho}\|_2^2 - \frac{1}{2\rho}\|\vu\|_2^2
\end{equation}

The term $-\frac{1}{2\rho}\|\vu\|_2^2$ can be ignored for the primal updates because it has no dependence on the primal variables. For this reason, it is sometimes more convenient to keep track of $\frac{\vu}{\rho}$ instead of $\vu$, since that is the form that appears in the augmented lagrangian after completing the square.

\begin{equation}
\frac{\vu^{(k + 1)}}{\rho} = \frac{\vu^{(k)}}{\rho} + \mA\vx^{(k + 1)} + \mB\vy^{(k + 1)} + \vc
\end{equation}

This form is known as scaled ADMM.

The other common variation of ADMM is distinct in that it updates the dual variable more frequently.
\begin{equation}
\vx^{(k + 1)} = \arg\min_{\vx} \operatorname{L}_{\rho}(\vx,\vy^{(k)},\vu^{(k)}
\end{equation}

\begin{equation}
\vu^{(k + \frac{1}{2})} = \vu^{(k)} + (\alpha - 1)\rho(\mA\vx^{(k + 1)} + \mB\vy^{(k)} + \vc)
\end{equation}

\begin{equation}
\vy^{(k + 1)} = \arg\min_{\vy} \operatorname{L}_{\rho}(\vx^{(k + 1)},\vy,\vu^{(k + \frac{1}{2})}
\end{equation}

\begin{equation}
\vu^{(k + 1)} = \vu^{(k + \frac{1}{2})} + \rho(\mA\vx^{(k + 1)} + \mB\vy^{(k + 1)} + \vc)
\end{equation}

When $\alpha > 1$, this is known as overrelaxation, and if $\alpha < 1$, this is known as under-relaxation.\footnote{I have chozen to notate over/under relaxation differently than what is standard, but the $\alpha$ is the same, and the notations are mathematically equivalent. The standard notation instead adds the term $-(1 - \alpha)(\mA\vx^{(k + 1)} + \mB\vy^{(k)} + \vc)$ to $\mA\vx^{(k + 1)}$ and substitutes that expression for $\mA\vx^{(k + 1)}$ in subsequent equations. I believe my notation is preferable in that it offers more insight to what $\alpha$ does in ADMM.} $\alpha$ is always chozen to be greater than zero. In some applications, researchers have found using over-relaxation with $\alpha \in \[1.5,1.8\]$ converges faster than without overrelaxation. For this reason, ADMM is commonly used with over-relaxation.


\section{Applying ADMM to the Sparse Coding Problem}
\section{Literature Review}
\subsection{Convolutional Sparse Coding}
\section{ADMM with Low-Rank Updates}
\section{Conclusion}

% This is a figure in landscape orientation
\begin{sidewaysfigure}
\includegraphics[width=\textwidth]{figures/exampleFigure.png}
\caption{This is another example Figure, rotated to landscape orientation.}
\label{LandscapeFigure}
\end{sidewaysfigure}
