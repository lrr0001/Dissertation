\chapter{Learning Dictionaries for Multi-Channel Signals}

\section{Introduction}
In a multi-layer dictionary model, the coefficients corresponding to a dictionary from one layer become the signal for the subsequent layer.  The number of channels for this signal is the number of dictionary filters from the previous layer.  Much of the literature on using convolutional dictionaries is tailored to applications with signals that only have a small number of channels.  This chapter presents a novel method to use learning convolutional dictionaries for multi-channel signals. 
\section{Dictionary Types}
There are many ways to construct a convolutional sparse representation of a multi-channel signal, but broadly the distinctions reduce down to if and how signal channels share dictionaries and coefficients, and if and how non-shared entities interact across channels.

It is common in many applications for dictionary models to share dictionaries across channels, which requires the use of multi-channel coefficients. If such models were used in a multi-layer dictionary model, the tensor rank would increase with each subsequent layer.

For this work, I focus instead on the multi-channel dictionary with shared coefficients. This structure matches that of convolutional neural networks, and the number of channels for a subsequent dictionary is the number of filters for the dictionary from the previous layer.
\section{Pursuit and Sparse Coding}
\label{section:sparse coding}
The dictionary model decomposes the signal $\vs_i$ into a dictionary $\mD$ (which generalizes to other signals) and the coefficients $\vx_i$ (which are specific to the signal $\vs_i$:
%
\begin{equation}
\vs_i \approx \mD\vx_i
\end{equation}
%
(Here the subscript $i$ specifies a particular signal and its corresponding coefficients.) A pursuit algorithm finds the coefficients $\vx_i$ corresponding to a particular signal $\vs_i$ for known dictionary $\mD$. If the number of dictionary atoms (columns) is larger than the dimension of the signal, then the number of unknowns is larger than the number of equations, and many solutions for $\vx_i$ represent $\vs_i$ equally well (at least in an L$2$ sense). Researchers and practitioners commonly either impose a sparsity constraint on the coefficients or add a coefficient L$1$ penalty to the objective function, which removes this ambiguity from the problem construction. When such a penalty or constraint is used, pursuit is sometimes called sparse coding. With the added coefficient L$1$ penalty, the pursuit optimization problem looks like this:
%
\begin{equation}\label{equation:sparse coding}
\vx_i = \arg\min_{\vx} \frac{1}{2}\|\vs_i - \mD\vx\|_2^2 + \lambda \|\vx\|_1
\end{equation}
%
where $\lambda$ is a nonnegative hyperparameter controlling how much the L$1$ norm of the coefficients is penalized. Researchers have proposed many ways to solve this problem. If the dictionary is convolutional and the number of channels is low, a standard approach is to use the Alternating direction Method of Multipliers (ADMM) algorithm.

\section{ADMM}
\label{section:ADMM}
ADMM is a convex-optimization algorithm used to solve the optimization problem:
\begin{equation} \label{equation:ADMM}
\begin{aligned}
\minimize_{\vx,\vy} & f(\vx) + g(\vy) \\
\text{subject to } & \mA\vx + \mB\vy + \vc = \vzero 
\end{aligned}
\end{equation}
%
where $f$ and $g$ are convex functions \cite{boyd2011distributed}. (I will address how to put the sparse coding problem in this form in the next section.)

The ADMM algorithm makes use of the augmented Lagrangian, a particular expression that has a saddle point at the solution to the constrained optimization problem:
%
\begin{equation}
\L_{\rho}(\vx,\vy,\vu) = f(\vx) + g(\vy) + \vu^H(\mA\vx + \mB\vy + \vc) + \frac{\rho}{2}\|\mA\vx + \mB\vy + \vc\|_2^2
\end{equation}
%
where $\rho$ is a hyperparameter greater than zero and $\vu$ is the dual variable for the constraints.

At the saddle-point solution, the augmented Lagrangian is at a minimum in respect to $\vx$ and $\vy$, but at a maximum in respect to $\vu$.

The ADMM algorithm is an iterative search for the saddle point of the augmented Lagrangian. Each iteration consists of a primal update for $\vx$, a primal update for $\vy$, and a dual update for $\vu$:
%
\begin{equation}
\vx^{(t + 1)} = \arg\min_{\vx} \L_{\rho}(\vx,\vy^{(t)},\vu^{(t)})
\end{equation}
%
\begin{equation}
\vy^{(t + 1)} = \arg\min_{\vy} \L_{\rho}(\vx^{(t + 1)},\vy,\vu^{(t)})
\end{equation}
%
\begin{equation} \label{equation:general dual update}
\vu^{(t + 1)} = \vu^{(t)} + \rho(\mA\vx^{(t + 1)} + \mB\vy^{(t + 1)} + \vc)
\end{equation}

\begin{algorithm}[H]
\SetAlgoLined
%\KwResult{Write here the result }
 %initialization:\\
   $\vy = $ inital guess \\
   $\vu = \vzero$ \\
 \While{Not Converged}{
  $\vx = \arg \min_{\vx}  \L_{\rho}(\vx,\vy,\vu)$ \\
  $\vy = \arg \min_{\vy} \L_{\rho}(\vx,\vy,\vu)$ \\
  $\vu = \vu + \rho(\mA\vx + \mB\vy + \vc)$
 }
 \caption{ADMM}
\end{algorithm}

The primal updates serve to move towards the minimum of the augmented Lagrangian in respect to $\vx$ and $\vy$ with $\vu$ fixed, and the dual update fixes $\vx$ and $\vy$, and performs gradient ascent on $\vu$ with stepsize $\rho$. Under very mild assumptions, this process converges to a saddle point of the augmented Lagrangian, which matches a solution to the constrained optimization problem.\footnote{Neither the saddle point nor the corresponding solution to the constrained optimization problem are guaranteed to be unique, however.}

There are two common variations of the ADMM algorithm that this dissertation will make use of.  The first is the scaled form, which comes from completing the square for the augmented lagrangian function:
%
\begin{equation}
\L_{\rho}(\vx,\vy,\vu) = f(\vx) + g(\vy) + \frac{\rho}{2}\|\mA\vx + \mB\vy + \vc + \frac{\vu}{\rho}\|_2^2 - \frac{\rho}{2}\|\frac{\vu}{\rho}\|_2^2
\end{equation}
%
The term $-\frac{\rho}{2}\|\frac{\vu}{\rho}\|_2^2$ can be ignored for the primal updates because it has no dependence on the primal variables. It is sometimes more convenient to keep track of $\frac{\vu}{\rho}$ instead of $\vu$, since that is the form that appears in the augmented Lagrangian after completing the square. The dual update for the scaled form is easily derived from equation \ref{equation:general dual update}.
%
\begin{equation}
\frac{\vu^{(t + 1)}}{\rho} = \frac{\vu^{(t)}}{\rho} + \mA\vx^{(t + 1)} + \mB\vy^{(t + 1)} + \vc
\end{equation}
%
This form is known as scaled ADMM.

Another common variation of ADMM updates the dual variable more frequently.
\begin{equation}
\vx^{(t + 1)} = \arg\min_{\vx} \L_{\rho}(\vx,\vy^{(t)},\vu^{(t)})
\end{equation}
%
\begin{equation}
\vu^{(t + \frac{1}{2})} = \vu^{(t)} + (\alpha - 1)\rho(\mA\vx^{(t + 1)} + \mB\vy^{(t)} + \vc)
\end{equation}
%
\begin{equation}
\vy^{(t + 1)} = \arg\min_{\vy} \L_{\rho}(\vx^{(t + 1)},\vy,\vu^{(t + \frac{1}{2})})
\end{equation}
%
\begin{equation}
\vu^{(t + 1)} = \vu^{(t + \frac{1}{2})} + \rho(\mA\vx^{(t + 1)} + \mB\vy^{(t + 1)} + \vc)
\end{equation}
%
When $\alpha > 1$, this is known as over-relaxation, and if $\alpha < 1$, this is known as under-relaxation.\footnote{I have elected to notate over/under relaxation differently than standard, but the $\alpha$ is the same, and the notations are mathematically equivalent. The standard notation does not use the first dual update, and instead includes another variable $\vh^{(t + 1)} = \mA\vx^{(t + 1)} - (1 - \alpha)(\mA\vx^{(t + 1)} + \mB\vy^{(t)} + \vc)$ and substitutes $\vh^{(t + 1)}$ for $\mA\vx^{(t + 1)}$ in the dual-update equation and the second primal-update equation. While more familiar to readers who have dealt with ADMM before, this standard notation complicates ADMM with an extra variable and obscures how the dual update and second primal update relate to the augmented Lagrangian.} $\alpha$ is always chozen to be greater than zero. In some applications, researchers have found using over-relaxation converges faster than without over-relaxation \cite{eckstein1994parallel}, but optimal choice of $\alpha$ is problem-dependent \cite{nishihara2015general}.

\begin{algorithm}[H]
\SetAlgoLined
%\KwResult{Write here the result }
 %initialization:\\
   $\alpha \in (0,2]$ \\
   $\vy = $ inital guess \\
   $\vu_s = \vzero$ \\
 \While{Not Converged}{
  $\vx = \arg \min_{\vx}  \L_{\rho}(\vx,\vy,\rho\vu_s)$ \\
  $\vu_s = \vu_s + (\alpha - 1)(\mA\vx + \mB\vy + \vc)$ \\
  $\vy = \arg \min_{\vy} \L_{\rho}(\vx,\vy,\rho\vu_s)$ \\
  $\vu_s = \vu_s + \mA\vx + \mB\vy + \vc$
 }
 \caption{Scaled ADMM With Over or Under-Relaxation}
\end{algorithm}


\section{Applying ADMM to the Sparse Coding Problem}
\label{section:Applying ADMM}
Recall from section \ref{section:sparse coding}, equation \ref{equation:sparse coding} for sparse coding.
%
\begin{equation}
\vx_i = \arg\min_{\vx} \frac{1}{2}\|\vs_i - \mD\vx\|_2^2 + \lambda \|\vx\|_1
\end{equation}
%
This can be rewritten to match the ADMM form from equation \ref{equation:ADMM}:
\begin{equation} \label{equation:SC Opt Prob}
\begin{aligned}
\minimize_{\vx,\vy} & \frac{1}{2} \|\vs_i - \mD\vx\|_2^2 + \lambda\|\vy\|_1 \\
         \text{subject to } & \vy - \vx = \vzero
\end{aligned}
\end{equation}
%
Given sufficient iterations, $\vx$ and $\vy$ will both be close to the optimal, but they may not be equal. Either can be used an approximate solution to the sparse coding problem.

Computing the augmented Lagrangian of the convex optimization problem in expression \ref{equation:SC Opt Prob} yields the following equation:
%
\begin{equation}
\L_{\rho}(\vx,\vy,\vu) = \frac{1}{2} \|\vs_i - \mD\vx\|_2^2 + \lambda\|\vy\|_1 + \frac{\rho}{2}\|\vy - \vx + \frac{\vu}{\rho}\|_2^2 - \frac{1}{2\rho}\|\vu\|_2^2
\end{equation}
%
Starting with the $\vx$-update:
\begin{equation}
\vx^{(t + 1)} = \arg\min_{\vx} \L_{\rho}(\vx,\vy^{(t)},\vu^{(t)})
\end{equation}
%
Since the desired result is the minimizer, setting the gradient to zero and solving for $\vx$ will produce the solution.
%
\begin{equation}
\nabla_{\vx^{(t + 1)}} \L_{\rho}(\vx^{(t + 1)},\vy{(t)},\vu{(t)}) = \vzero
\end{equation}
%
\begin{equation}
\vzero = \mD^T\mD\vx^{(t + 1)} - \mD^T\vs_i + \rho\vx^{(t + 1)} - \rho\left(\vy^{(t)} + \frac{\vu^{(t)}}{\rho}\right)
\end{equation}
%
\begin{equation}
(\rho\mId + \mD^T\mD)\vx^{(t + 1)} = \mD^T\vs_i + \rho\left(\vy^{(t)} + \frac{\vu^{(t)}}{\rho}\right)
\end{equation}
%
\begin{equation}
\vx^{(t + 1)} = (\rho\mId + \mD^T\mD)^{-1}\left(\mD^T\vs_i + \rho\left(\vy^{(t)} + \frac{\vu^{(t)}}{\rho}\right)\right)
\end{equation}
%
In subsection \ref{subsection:SC xupdate}, there is a discussion of the implications of this update equation, how to compute it for cases in which the signal has a low number of channels, and the challenges it poses for signals with many channels.

If using over-relaxation\footnote{or under-relaxation}, there is a dual update:
\begin{equation}
\frac{\vu^{(t + \frac{1}{2})}}{\rho} = \frac{\vu^{(t)}}{\rho} + (\alpha - 1)(\vy^{(t)} - \vx^{(t + 1)})
\end{equation}
%
Moving on to the $\vy$-update:
\begin{equation}
\vy^{(t + 1)} = \arg\min_{\vy} \operatorname{L}_{\rho}(\vx^{(t + 1)},\vy,\vu^{(t + \frac{1}{2})})
\end{equation}
%
Excluding the terms that don't include $\vy$ yields
\begin{equation}
\vy^{(t + 1)} = \arg\min_{\vy} \lambda\|\vy\|_1 + \frac{\rho}{2}\|\vy - \vx^{(t + 1)} + \frac{\vu^{(t + \frac{1}{2})}}{\rho}\|_2^2
\end{equation}
%
This is a well-known problem, whose solution is
\begin{equation}
\vy^{(t + 1)} = \operatorname{S}_{\frac{\lambda}{\rho}}(\vx^{(t + 1)} - \frac{\vu^{(t + \frac{1}{2})}}{\rho})
\end{equation}
where $\operatorname{S}$ is the shrinkage operator:
\begin{equation}
\operatorname{S}_{b}(x) = \begin{cases} x - b & x > b \\ 0 & -b < x < b \\ x + b & x < - b \end{cases}
\end{equation}
In the case of a vector, matrix, or tensor input, the shrinkage operator is applied element by element.

Finally, the last update equation for the dual variable:
\begin{equation}
\frac{\vu^{(t + 1)}}{\rho} = \frac{\vu^{(t + \frac{1}{2})}}{\rho} + \vy^{(t + 1)} - \vx^{(t + 1)}
\end{equation}

\begin{algorithm}[H]
\SetAlgoLined
%\KwResult{Write here the result }
 %initialization:\\
   $\alpha \in (0,2]$ \\
   $\vy = \mD^T\vs$ \\
   $\vu_s = \vzero$ \\
 \While{Not Converged}{
  $\vx = \left(\rho\mId + \mD^T\mD\right)^{-1}\left(\mD^T\vs + \rho(\vy + \vu_s)\right)$ \\
  $\vu_s = \vu_s + (\alpha - 1)(\vy - \vx)$ \\
  $\vy = \operatorname{S}_{\frac{\lambda}{\rho}}(\vx - \vu_s)$ \\
  $\vu_s = \vu_s + \vy - \vx$
 }
 \caption{ADMM for Sparse Coding}
\end{algorithm}

\subsection{Exploiting Dictionary Structure for the Inverse Problem}
\label{subsection:SC xupdate}
Returning to the $\vx$ update:
\begin{equation}
\vx^{(t + 1)} = \left(\rho\mId + \mD^T\mD\right)^{-1}\left(\mD^T\vs_i + \rho(\vy^{(t)} + \frac{\vu^{(t)}}{\rho})\right)
\end{equation}
%
For problems using a dictionary with convolutional structure, this inverse for the convolutional sparse coding problem is very structured. Exploiting this structure is important for efficient computation, because the matrix $\rho\mId + \mD^T\mD$ is a large matrix.

Writing $\mD$ in a block structure, I have
%
\begin{equation}
\mD = \begin{bmatrix}
\mD_{1,1}, & \hdots, & \mD_{1,M}\\
\vdots & \ddots & \vdots\\
\mD_{C,1}, & \hdots, & \mD_{C,M}
\end{bmatrix}
\end{equation}
%
where $\mD_{c,m}$ is a toplitz matrix capturing channel $c$ of the $m$th filter of the dictionary. Toplitz matrices are diagonalizable with Fourier eigenvectors:
%
\begin{equation}
\mD = \begin{bmatrix}
\F^{-1}\hat{\mD}_{1,1}\F, & \hdots, & \F^{-1}\hat{\mD}_{1,M}\F\\
\vdots & \ddots & \vdots\\
\F^{-1}\hat{\mD}_{C,1}\F, & \hdots, & \F^{-1}\hat{\mD}_{C,M}\F
\end{bmatrix}
\end{equation}
%
where $\hat{\mD}_{c,m}$ is a diagonal matrix whose elements are the discrete Fourier transform (FFT) of channel $c$ of the $m$th dictionary filter.

This sparsely banded structure is a useful form in analyzing the structure of the inverse problem:
\begin{equation}
(\rho \mId + \mD^T\mD)^{-1} = \F^{-1}(\rho\mId + \hat{\mD}^H\hat{\mD})^{-1}\F
\end{equation}
%
where
\begin{equation}
\hat{\mD} = \begin{bmatrix}
\hat{\mD}_{1,1}, & \hdots, & \hat{\mD}_{1,M}\\
\vdots & \ddots & \vdots\\
\hat{\mD}_{C,1}, & \hdots, & \hat{\mD}_{C,M}
\end{bmatrix}
\end{equation}
%
and in a slight abuse of notation, $\F$ computes the FFT separately on the coefficients for each filter.  In \cite{Bristow2013fast}, Bristow et al. observe the matrix $\rho\mId + \hat{\mD}^H\hat{\mD}$ is sparsely banded, so the inverse can be broken down into much smaller inverse problems, and one only needs to compute the inverse of an $M \times M$ matrix for every element in the signal. ($\rho\mId + \hat{\mD}^H\hat{\mD}$ is an $M \times M$ block matrix, whose blocks are diagonal.  Each submatrix collects one element from the diagonal of each of the blocks.)

Furthermore, the maximum rank of these submatrices is $C$, so if $C$ is small, these inverses can be computed even more efficiently using the Woodbury matrix identity or Sherman-Morrison equations \cite{vsorel2016fast} \cite{heide2015fast} \cite{wohlberg2015efficient}.

According to the Woodbury matrix identity \cite{henderson1981deriving}, for any invertible matrix $\mU$ and any matrix $\mV$:
\begin{equation}
(\mU + \mV^H\mV)^{-1} = \mU^{-1} - \mU^{-1}\mV^H(\mId + \mV\mU^{-1}\mV^H)^{-1}\mV\mU^{-1}
\end{equation}
%
So,
\begin{equation}
(\rho\mId + \hat{\mD}^H\hat{\mD})^{-1} = \frac{1}{\rho}\mId - \frac{1}{\rho}\hat{\mD}^H(\rho\mId + \hat{\mD}\hat{\mD}^H)^{-1}\hat{\mD}
\end{equation}
%
This means that instead of computing the inverse of an $M \times M$ matrix for every pixel in the image, one could instead choose to compute the inverse of a $C \times C$ matrix for each pixel in the image.\footnote{Generally, Cholesky or LDLT decomposition would be preferable to explicitly computing the inverse, and the efficiency gains due to the Woodbury matrix identity are relevant regardless of the chosen representation.}

\section{Sparse Coding for Multi-Channel Signals: Alternatives to My Novel Approach}
In applying ADMM to the convolutional sparse coding problem, \cite{vsorel2016fast} \cite{heide2015fast} \cite{wohlberg2015efficient} exploit the low-rank structure of the inverse problem in the $\vx$ update for efficient computation. Unfortunately, this relies on the number of channels being small. Broadly, there are two main approaches to avoid or simplify this challenging inverse problem: either construct a variant of the ADMM algorithm that simplifies the inverse problem, or use a proximal gradient approach that avoids it altogether.

In \cite{chodosh2018deep}\cite{murdock2018deep}, the authors use the ADMM algorithm for sparse coding. They observe that if the dictionary is a tight frame, that is, $\mD\mD^T = \mId$, then the inverse can be simplied without using the frequency representation.
%
\begin{equation}
(\rho\mId + \mD^T\mD)^{-1} = \frac{1}{\rho}\mId - \frac{1}{\rho(\rho + 1)}\mD^T\mD
\end{equation}
%
This produces the $\vx$ update equation:
%
\begin{equation}
\vx^{(t + 1)} = \frac{1}{\rho + 1}\mD^T\vs + \left(\mId - \frac{1}{\rho + 1}\mD^T\mD\right)\left(\vz^{(t)} - \frac{\vgamma^{(t)}}{\rho}\right)
\end{equation}
%
The above equations can be derived using the Woodbury matrix identity. In their work, they use the equations built on the assumption that the dictionary is a tight frame, but develop no mechanism to ensure that their assumption is accurate. Thus, ultimately $\frac{1}{\rho}\mId - \frac{1}{\rho(\rho + 1)}\mD^T\mD$ merely serves as an approximation to $(\rho\mId + \mD^T\mD)^{-1}$. Empirically, they that observe the algorithm converges, but the dictionaries they learn are not tight frames, so the solution they converge to is not optimal\footnote{The solution does not minimize the sparse coding objective function.}.

Other works avoid the ADMM algorithm entirely.

The iterative shrinkage thresholding algorithm (ISTA) is an iterative algorithm that minimizes the sum of two convex functions $f$ and $g$. $f$ is required to be smooth. It is helpful for $f$ to be easily differentiable and $g$ to have a simple proximal operator.
%
\begin{equation}
\prox_g(\vmu) = \arg \min_{\vnu} \frac{1}{2}\|\vnu - \vmu\|_2^2 + g(\vnu)
\end{equation}
%
Then, ISTA has the following update equation, where the constant $\mathbb{L}$ constrols step size.
%
\begin{equation}
\vx^{(t + 1)} = \prox_{\frac{g}{\mathbb{L}}}\left(\vx^{(t)} - \frac{1}{\mathbb{L}}\nabla_{\vx} f(\vx^{(t)})\right)
\end{equation}
%
FISTA is similar to ISTA, but adds momentum \cite{beck2009fast}.
\begin{equation}
\vz^{(t + 1)} = \prox_{\frac{g}{\mathbb{L}}}\left(\vx^{(t)} - \frac{1}{\mathbb{L}}\nabla_{\vx}f(\vx^{(t)})\right)
\end{equation}
\begin{equation}
r^{(t + 1)} = \frac{1}{2}\left(1 + \sqrt{1 + 4(r^{(t)})^2}\right)
\end{equation}
\begin{equation}
\vx^{(t + 1)} = \vz^{(t + 1)} + \frac{r^{(t)} - 1}{r^{(t + 1)}}(\vz^{(t + 1)} - \vx^{(t)})
\end{equation}
%
Applying FISTA to the sparse coding problem, $\frac{1}{2}\|\vs - \mD\vx\|_2^2$ is straightforward to differentiate and $\lambda\|\vx\|_1$ has a simple proximal operator.
%
\begin{equation}
\nabla_{\vx}\left(\frac{1}{2}\|\vs - \mD\vx\|_2^2\right) = \mD^T\mD\vx - \mD^T\vs
\end{equation}
%
\begin{equation}
\prox_{\frac{\lambda}{\mathbb{L}}\|\cdot\|_1}(\cdot) = \operatorname{S}_{\frac{\lambda}{\mathbb{L}}}
\end{equation}
%
So, the FISTA equations for convolutional basis pursuit are the following:
\begin{equation}
\vz^{(t + 1)} = \operatorname{S}_{\frac{\lambda}{\mathbb{L}}}\left(\vx^{(t)} - \frac{1}{\mathbb{L}}\mD^T(\mD\vx^{(t)} - \vs)\right)
\end{equation}
\begin{equation}
r^{(t + 1)} = \frac{1}{2}\left(1 + \sqrt{1 + 4(r^{(t)})^2}\right)
\end{equation}
\begin{equation}
\vx^{(t + 1)} = \vz^{(t + 1)} + \frac{r^{(t)} - 1}{r^{(t + 1)}}(\vz^{(t + 1)} - \vx^{(t)})
\end{equation}
%
In \cite{wohlberg2015efficient}, Wohlberg compares FISTA to ADMM on a sparse coding task and finds FISTA converges much slower than ADMM. However, the comparison is made on signals with few channels, so ADMM is able to exploit the structure of $\mD$ for efficient $\vx$ updates.

In a recent work \cite{chodosh2020use}, Chodosh and Lucey derive prox-linear updates using convex solver methods detailed in \cite{xu2013block}.

The updates come from the formula:
\begin{equation}
\vz^{(t + 1)} = \arg\min_{\vz} \left(\nabla f(\vx^{(t)})\right)^T(\vz - \vx^{(t)}) + \frac{\mathbb{L}}{2}\|\vz - \vx^{(t)}\|_2^2 + \lambda\|\vz\|_1
\end{equation}
%
where $\vx^{(t)} = \vz^{(t)} + \omega_t(\vz^{(t)} - \vz^{(t - 1)})$ and $\omega_t$ is a momentum factor.

This yields the update equation\footnote{In their paper, they add a non-negativity constraint and allow different $\lambda$ for the coefficients of each filter (and possibly spatially varied as well). They also are constucting the equations specifically for a multi-layer network. I simplified their equations to illistrate how their approach relates to the FISTA algorithm.}:
\begin{equation}
\vz^{(t + 1)} = \operatorname{S}_{\frac{\lambda}{\mathbb{L}}}\left(\vx^{(t)} - \frac{1}{\mathbb{L}}\mD^T(\mD\vx^{(t)} - \vs)\right)
\end{equation}
%
\begin{equation}
\vx^{(t + 1)} = \vz^{(t + 1)} + \omega_t(\vz^{(t + 1)} - \vz^{(t)})
\end{equation}
%
While neither Chodosh and Lucey, nor the work they cite, mention FISTA, the resemblance is very close. There are two distinctions:
\begin{enumerate}
\item Momentum is computed slightly differently: to match FISTA, the prox-linear updates would need to use $\vx^{(t)} - \bar{\vx}^{(t - 1)}$ for momentum. Instead, they use $\vx^{(t)} - \vx^{(t - 1)}$.
\item The prox-linear approach scales the momentum steps differently.
\end{enumerate}

Given these similarities, it is likely the performance between the two methods is similar.

\section{Dictionary Learning}
The last few sections have focused on sparse coding. For sparse coding, the dictionary is fixed or known. Most dictionary learning algorithms alternate between pursuing coefficients and updating dictionary filters, as shown in algorithms \ref{Algorithm: Online Dictionary Learning} and \ref{Algorithm: Batch Dictionary Learning}. The CoeficientUpdate in algorithm \ref{Algorithm: Batch Dictionary Learning} may consist of one or more sparse coding update steps. Not all dictionary learning algorithms perfectly adhere to this stucture, but the template covers most approaches.
\begin{algorithm}[h] \label{Algorithm: Online Dictionary Learning}
\SetAlgoLined
%\KwResult{Write here the result }
 %initialization:\\
   $\mD = \mD_{\text{init}}$ \\
   $i = 0$
 \While{Stopping Criteria Not Met}{
  $\vs = \operatorname{GetData}(i)$ \\
  $\vx = \operatorname{Pursuit}(\mD,\vs)$ \\
  $\mD = \operatorname{DictionaryUpdate}(\mD,\vs,\vx)$ \\
  $i = i + 1$
 }
 \caption{Online Dictionary Learning Algorithm}
\end{algorithm}
\begin{algorithm}[h] \label{Algorithm: Batch Dictionary Learning}
\SetAlgoLined
%\KwResult{Write here the result }
 %initialization:\\
   $\mD = \mD_{\text{init}}$ \\
   $\mX = \mX_{\text{init}}$ \\
 \While{Stopping Criteria Not Met}{
  $\mX = \operatorname{CoeficientUpdate}(\mD,\mX,\mS)$ \\
  $\mD = \operatorname{DictionaryUpdate}(\mD,\mS,\mX)$
 }
 \caption{Batch Dictionary Learning Algorithm}
\end{algorithm}

There are many methods to updating dictionaries: FISTA, ADMM, projected stochastic gradient descent, et cetera. Dictionary updates generally adhere to the following three properties:
\begin{enumerate}
\item The updated dictionary filters better represents the data for the given coefficients.\label{item:improvement property}
\begin{equation}
\|\mS - \mD^{(n + 1)}\mX\|_F^2 \leq \|\mS - \mD^{(n)}\mX\|_F^2
\end{equation}
\item The updated dictionary filters are normalized.\label{item:normalized}
\begin{equation}
\sum_{c = 1}^C \|\vd_{c,m}\|_2^2 = 1
\end{equation}
\item The updated dictionary filters are spatially constrained. \label{item:spatially constrained}
\begin{equation}
(\mId - \mT)\vd_{c,m} = \vzero
\end{equation}
(Here, $\mId - \mT$ selects the elements of $\vd_{c,m}$ that must be zero.)
\end{enumerate}
Property \ref{item:spatially constrained} relates to the idea that convolutional filters are usually small $5 \times 5$ or $9 \times 9$, et cetera. Dictionary updates do not increase the size of the filters.

If using ADMM for pursuit, every time the dictionary is updated, the inverse representation must be updated as well. The inverse representation can be updated more efficiently if the dictionary update is low rank, but approximating the dictionary update using a truncated singular value decomposition would forfeit property \ref{item:normalized}. The next section describes a novel means to work handle these challenges, with an explanation of how to efficiently update the inverse representation and a sparse coding method designed to handle dictionary updates that produce unnormalized dictionaries.

\section{A Novel Approach to Sparse Coding: ADMM with Low-Rank Dictionary Updates}
In this section, I present a novel approach to sparse coding for signals with a large number of channels. The approach uses the ADMM algorithm described in section \ref{section:ADMM} and will share many similarities to the standard ADMM sparse coding approach described in section \ref{section:Applying ADMM} for signals with few channels.

\subsection{Updating the Inverse Representation}

Under many circumstances, inverse representations can be updated efficiently, provided the update adheres to a low-rank structure. Recall the freqeuncy representation of the convolutional dictionary:
%
\begin{equation}
\hat{\mD} = \begin{bmatrix}
\hat{\mD}_{1,1}, & \hdots, & \hat{\mD}_{1,M}\\
\vdots & \ddots & \vdots\\
\hat{\mD}_{C,1}, & \hdots, & \hat{\mD}_{C,M}
\end{bmatrix}
\end{equation}
%
where $\hat{\mD}_{c,m}$ is diagonal for all $c$ and $m$. Let $\hat{\mD}_{c,m}[\hat{k}]$ be the $\hat{k}$th element of the diagonal and let
%
\begin{equation}
\hat{\mD}[\hat{k}] = \begin{bmatrix}
\hat{\mD}_{1,1}[\hat{k}], & \hdots, & \hat{\mD}_{1,M}[\hat{k}]\\
\vdots & \ddots & \vdots\\
\hat{\mD}_{C,1}[\hat{k}], & \hdots, & \hat{\mD}_{C,M}[\hat{k}]
\end{bmatrix}
\end{equation}
%
Then $\hat{\mD}[\hat{k}]$ is a $C \times M$ matrix collecting the $\hat{k}$th freqency of all channels and filters of $\mD$.

Thus, $(\rho\mId + \hat{\mD}^H\hat{\mD})^{-1}$ really consists of $\hat{k}$ separate inverse problems: $(\rho\mId + \hat{\mD}^H[\hat{k}]\hat{\mD}[\hat{k}])^{-1}$.

Consider the update equation.
%
\begin{equation}
\hat{\mD}[\hat{k}]^{(n + 1)} = \hat{\mD}[\hat{k}]^{(n)} + \mU\mV[\hat{k}]^H
\end{equation}
%
where $\mU$ is an orthogonal matrix of size $C \times \mathbb{R}$ and $\mV[\hat{k}]$ is an orthogonal matrix of size $M \times \mathbb{R}$.\footnote{$\mU$ and $\mV$ are also iteration specific, but to notate that would over-clutter the equations. For efficient, low-rank updates to the inverse representation, I could allow both $\mU$ and $\mV$ to vary in respect to frequency $\hat{k}$ (instead of just $\mV$). However, I also need to limit the spatial support of the dictionary (so that the filter size is small), and preventing $\mU$ from varying across frequency is part of a means to satisfy that constraint.}

Then,
%
\begin{equation}
\rho\mId + (\hat{\mD}^{(n + 1)})^H[\hat{k}]\hat{\mD}^{(n + 1)}[\hat{k}] = \rho\mId + (\mD^{(n)}[\hat{k}] + \mU\mV^H[\hat{k}])^H(\mD^{(n)}[\hat{k}] + \mU\mV^H[\hat{k}])
\end{equation}
%
For brevity and simplicity, I will drop the notation indexing the frequency $\hat{k}$ and selecting the iteration $n$ for matrix $\hat{\mD}^{(n)}[\hat{k}]$ and simply use $\hat{\mD}$ instead. However, the reader should keep in mind the $\hat{\mD}$ here is a dense $C \times M$ matrix capturing the component of the dictionary $\mD$ for implicit frequency $\hat{k}$, only a submatrix of the sparsely banded $\hat{\mD}$ of size $KC \times M$ from earlier in this section.
%
\begin{equation}
\rho\mId + (\hat{\mD}^{(n + 1)})^H\hat{\mD}^{(n + 1)} = \rho\mId + (\hat{\mD}^{(n)})^H\hat{\mD}^{(n)} + \mV\mU^H \mU\mV^H + \mV\mU^H\hat{\mD}^{(n)} + (\hat{\mD}^{(n)})^H\mU\mV^H
\end{equation}
%
Given that $\mV$ and $\mU$ are orthogonal matrices, $\mV\mU^H\mU\mV^H$ can easily be broken into $\mathbb{R}$ rank-one Hermitian updates.
%
\begin{equation}
\mV\mU^H\mU\mV^H = \sum_{\ell = 1}^{\mathbb{R}} \vu_{\ell}^H\vu_{\ell}\vv_{\ell}\vv_{\ell}^H
\end{equation}
%
Similarly, $\mV\mU^H\hat{\mD} + \hat{\mD}^H\mU\mV^H$ can be broken into $\mathbb{R}$ Hermitian, rank-two updates:
%
\begin{equation}
\mV\mU^H\hat{\mD} + \hat{\mD}^H\mU\mV^H = \sum_{\ell = 1}^{\mathbb{R}} \left( \vv_{\ell}\vu_{\ell}^H\hat{\mD} + \hat{\mD}^H\vu_{\ell}\vv_{\ell}^H \right)
\end{equation}
%
%
Inverse representations can be efficiently updated if the update is Hermitian and rank one. The details of such updates are discussed in Appendix \ref{chapter:Cholesky}.

The Hermitian rank-two update consists of two rank-one terms, but the terms are not Hermitian, complicating the update process. However, this can be resolved through eigendecomposition.  
%
\begin{equation}
\vv_{\ell}\vu_{\ell}^H \hat{\mD} + \hat{\mD}\vu_{\ell}\vv_{\ell}^H = \begin{bmatrix}
\vv_{\ell} & \hat{\mD}^H\vu_{\ell} 
\end{bmatrix}
\begin{bmatrix}
\hat{\mD}^H\vu_{\ell} & \vv_{\ell}
\end{bmatrix}^H
\end{equation}
%
While matrix products are not communitive, some of the eigenvalues of matrix products are communitive.

Furthermore, for general matrices $\mA$ and $\mB$ the eigenvectors of $\mA\mB$ and $\mB\mA$ are related:
%
\begin{equation}
\mB\mA\vx = \lambda\vx  \implies  \mA\mB\mA\vx = \lambda\mA\vx
\end{equation}
%
where $\lambda$ is the eigenvalue and $\vx$ is a vector.\footnote{I appologize for the reuse of certain variables here. Please do not confuse this $\vx$ for the sparse coding coefficients, $\lambda$ for the L$1$ penalty factor applied to the coefficients, or $\mA$ and $\mB$ for the matrices in the ADMM constraints.}

So, if $\vx$ is an eigenvector of $\mB\mA$, $\mA\vx$ is an eigenvector of $\mA\mB$.
%
%
\begin{equation}
\begin{bmatrix}
\hat{\mD}^H\vu_{\ell} & \vv_{\ell}
\end{bmatrix}^H
\begin{bmatrix}
\vv_{\ell} & \hat{\mD}^H\vu_{\ell} 
\end{bmatrix}
 = 
\begin{bmatrix}
\vu_{\ell}^H\hat{\mD}\vv_{\ell} & \vu_{\ell}^H\hat{\mD}\hat{\mD}^H\vu_{\ell} \\
\vv_{\ell}^H\vv_{\ell}    & \vv_{\ell}^H\hat{\mD}^H\vu_{\ell}
\end{bmatrix}
\end{equation}
%
The eigenvalues and corresponding eigenvectors of a $2 \times 2$ matrix can be computed using the quadradic formula. Assuming that the $2 \times 2$ matrix has $2$ distinct eigenvalues, the expressions for these are below.\footnote{It is not guarenteed the $2 \times 2$ matrix will have $2$ distinct eigenvalues. In Appendix \ref{chapter:eigenedge}, I consider those cases.}
%
\begin{equation}
\operatorname{eigval}\left(\begin{bmatrix} a & b \\ c & a^* \end{bmatrix}\right) = \operatorname{real}(a) \pm \sqrt{bc - (\operatorname{imag}(a))^2}
\end{equation}
%
\begin{equation}
\operatorname{eigvec}\left(\begin{bmatrix} a & b \\ c & a^* \end{bmatrix}\right) = \begin{bmatrix} b \\ -j\operatorname{imag}(a) \pm \sqrt{bc - (\operatorname{imag}(a))^2}\end{bmatrix}
\end{equation}
%
For the sake of brevity, I will drop the subscripts for $\vu$ and $\vv$.

Letting $\eta_{\vu} = \|\hat{\mD}^H\vu\|_2^2$, $\eta_{\vv} = \|\vv\|_2^2$, and $\eta_{\vu,\vv} = \vu^H\hat{\mD}\vv$:
%
\begin{equation}
\operatorname{eigval}\left(\begin{bmatrix}
\eta_{\vu,\vv} & \eta_{\vu} \\
\eta_{\vv}    & \eta_{\vu,\vv}^*
\end{bmatrix}\right)
= \operatorname{real}(\eta_{\vu,\vv}) \pm \sqrt{\eta_{\vv}\eta_{\vu} - (\operatorname{imag}(\eta_{\vu,\vv}))^2}
\end{equation}
%
\begin{equation}
\operatorname{eigvec}
\left(\begin{bmatrix}
\eta_{\vu,\vv} & \eta_{\vu} \\
\eta_{\vv}    & \eta_{\vu,\vv}^*
\end{bmatrix}\right)
= \begin{bmatrix}
\eta_{\vu} \\
-j\operatorname{imag}(\eta_{\vu,\vv}) \pm \sqrt{\eta_{\vv}\eta_{\vu} - (\operatorname{imag}(\eta_{\vu,\vv}))^2}
\end{bmatrix}
\end{equation}
%
%
%
Therefore, 
%
\begin{equation}
\operatorname{eigvec}
(
\vv\vu^H\hat{\mD} + \hat{\mD}^H\vu\vv^H
) = 
\eta_{\vu}\vv +
\left(-j\operatorname{imag}(\eta_{\vu,\vv}) \pm \sqrt{\eta_{\vv} \eta_{\vu} - (\operatorname{imag}(\eta_{\vu,\vv}))^2}\right)\hat{\mD}^H\vu
\end{equation}
%
%
\begin{equation}
\operatorname{eigval}(
\vv\vu^H\hat{\mD} + \hat{\mD}^H\vu\vv^H
)
= \operatorname{real}(\eta_{\vu,\vv}) \pm \sqrt{\eta_{\vv}\eta_{\vu} - (\operatorname{imag}(\eta_{\vu,\vv}))^2}
\end{equation}
%
This decomposition splits the Hermitian rank-two update into two Hermitian rank-one updates that can be used to update the inverse representation for $\rho\mId + \hat{\mD}^H[\hat{k}]\hat{\mD}[\hat{k}]$.

Recall once again, the update under consideration:
\begin{equation}
\hat{\mD}^{(n + 1)}[\hat{k}] = \hat{\mD}^{(n)}[\hat{k}] + \mU\mV^H[\hat{k}]
\end{equation}
%
This update must be of rank $\mathbb{R}$ at every frequency. Furthermore, the dictionary filter is spatially limited to its filter size. This second constraint is met if $\mV^H[\hat{k}]$ is similarly spatially limited.

\subsection{Handling Dictionary Normalization}
Consider the optimization problem:
%
\begin{equation}
\begin{aligned}
\min_{\vx,\vy} & \frac{1}{2}\|\vs -\mD\vx\|_2^2 + \lambda\|\vy\|_1 \\
\text{subject to } & \mR^{-1}\vy - \mR^{-1}\vx = 0 
\end{aligned}
\end{equation}
%
where $\mR$ is a diagonal matrix with scaled identity blocks:
%
\begin{equation}
\mR = \begin{bmatrix} r_1\mId & \vzero & \hdots & \vzero \\ \vzero & r_2\mId &  & \vdots \\ \vdots &  & \ddots &   \\ \vzero & \hdots &  & r_M\mId \end{bmatrix}
\end{equation}
%
and $\mD$ has normalized dictionary filters.

This optimization problem has the augmented Lagrangian function:
%
\begin{equation}
\L_{\rho}(\vx,\vy,\vu) = \frac{1}{2}\|\vs - \mD\vx\|_2^2 + \lambda\|\vy\|_1 + \vu^H\mR^{-1}(\vy - \vx)  + \frac{\rho}{2}\|\mR^{-1}(\vy - \vx)\|_2^2
\end{equation} 
%
\begin{equation}
\nabla_{\vx}\operatorname{L}_{\rho}(\vx,\vy,\vu) = -\mR^{-1}\vu - \mD^H\vs + \mD^T\mD\vx + \rho\mR^{-2}\vx - \rho\mR^{-2}\vy
\end{equation}
%
For $\vx,\vy,\vu$ such that $\nabla_{\vx}\L_{\rho}(\vx,\vy,\vu) = 0$:
%
\begin{equation}
(\rho\mR^{-2} + \mD^T\mD)\vx = \rho\mR^{-2}\vy + \mR^{-1} \vu + \mD^T\vs
\end{equation}
%
\begin{equation}
\mR^{-1}\left(\rho\mId + (\mD\mR)^T(\mD\mR)\right)\mR^{-1}\vx = \rho\mR^{-2}\vy + \mR^{-1}\vu + \mD^T\vs
\end{equation}
%
\begin{equation}
\left(\rho\mId + (\mD\mR)^T(\mD\mR)\right)\mR^{-1}\vx = \rho\mR^{-1}\vy + \vu + (\mD\mR)^T\vs
\end{equation}
%
\begin{equation}
\mR^{-1}\vx = \left(\rho\mId + (\mD\mR)^H(\mD\mR)\right)^{-1}\left(\rho\mR^{-1}\vy + \vu + (\mD\mR)^T\vs\right)
\end{equation}
%
So,
%
\begin{equation}
\arg\min_{\vx} \L_{\rho}(\vx,\vy,\vu) = \mR\left(\rho\mId + (\mD\mR)^T(\mD\mR)\right)^{-1}\left(\rho\mR^{-1}\vy + \vu + (\mD\mR)^T\vs\right)
\end{equation}
%
However, taking a similar approach to that of scaled ADMM, it will be simpler to track $\mR^{-1}\vx$ instead of $\vx$ directly.
%
\begin{equation}
\mR^{-1}\vx^{(t + 1)} = \left(\rho\mId + (\mD\mR)^T(\mD\mR)\right)^{-1}\left((\mD\mR)^T\vs + \rho\left(\mR^{-1}\vy^{(t)} + \frac{\vu^{(t)}}{\rho}\right)\right)
\end{equation}
%
Moving on to the $\vy$ update,
%
\begin{equation}
\arg\min_{\vy} \L_{\rho}(\vx,\vy,\vu) = \operatorname{S}_{\frac{\lambda\mR^2}{\rho}}\left(\vx - \frac{\mR\vu}{\rho}\right)
\end{equation}
%
\begin{equation}
\mR^{-1}\vy^{(t + 1)}= \operatorname{S}_{\frac{\lambda\mR}{\rho}}\left(\mR^{-1}\vx^{(t + 1)} - \frac{\vu^{\left(t + \frac{1}{2}\right)}}{\rho}\right)
\end{equation}
%
Finally, the dual updates are 
\begin{equation}
\frac{\vu^{\left(t + \frac{1}{2}\right)}}{\rho} = \frac{\vu^{(t)}}{\rho} + (\alpha - 1)(\mR^{-1}\vy^{(t)} - \mR^{-1}\vx^{(t + 1)})
\end{equation}
%
\begin{equation}
\frac{\vu^{(t + 1)}}{\rho} = \frac{\vu^{\left(t + \frac{1}{2}\right)}}{\rho} + \mR^{-1}\vy^{(t + 1)} - \mR^{-1}\vx^{(t + 1)}
\end{equation}
%
Thus, with this modification to the sparse coding optimization problem, the inverse representation used in the $\vx$ updates can be updated efficiently (given that the dictionary updates adhere to a particular low-rank structure), and normalization can be handed through a normalization factor $\mR^{-1}$.


\begin{algorithm}[h]
\SetAlgoLined
 %initialization:\\
   $\alpha \in (0,2]$ \\
   $\mD_s = \mD\mR$  \\
   $\vy_s = \mR^{-2}\mD_s^T\vs$ \\
   $\vu_s = \vzero$ \\
 \While{Not Converged}{
  $\vx_s = \left(\rho\mId + \mD_s^T\mD_s\right)^{-1}\left(\mD_s^T\vs + \rho\left(\vy_s + \vu_s\right)\right)$ \\
  $\vu_s = \vu_s + (\alpha - 1)(\vy_s - \vx_s)$ \\
  $\vy_s = \operatorname{S}_{\frac{\lambda\mR}{\rho}}(\vx_s - \vu_s)$ \\
  $\vu_s = \vu_s + \vy_s - \vx_s$
 }
  $\vx = \mR\vx_s$ \\
  $\vy = \mR\vy_s$
 \caption{ADMM for Sparse Coding, Large Number of Channels}
\end{algorithm}

\begin{algorithm}[h]
\SetAlgoLined
 %initialization:\\
   \tcp{Initialization:}
   $\alpha \in (0,2]$ \\
   $\mD\mR = \mD_{init}$ \\
   \For{$m \in \{1,\hdots M\}$}{
      $\mR_{m,m} = \|(\mD\mR)_m\|_F$
   }
   $i = 0$
   \While{Stopping Criteria Not Met}{
   $\vs = \operatorname{GetData}(i)$ \\
   \tcp{Pursuit:}
   $\mD_s = \mD\mR$  \\
   $\vy_s = \mR^{-2}\mD_s^T\vs$ \\
   $\vu_s = \vzero$ \\
 \While{Not Converged}{
  $\vx_s = \left(\rho\mId + \mD_s^T\mD_s\right)^{-1}\left(\mD_s^T\vs + \rho\left(\vy_s + \vu_s\right)\right)$ \\
  $\vu_s = \vu_s + (\alpha - 1)(\vy_s - \vx_s)$ \\
  $\vy_s = \operatorname{S}_{\frac{\lambda\mR}{\rho}}(\vx_s - \vu_s)$ \\
  $\vu_s = \vu_s + \vy_s - \vx_s$
 }
  $\vx = \mR\vx_s$ \\
  $\vy = \mR\vy_s$  \\
   \tcp{Dictionary Update:} 
   $\tilde{\mD} = \mT(\mD\mR - \alpha\nabla_{\mD}f(\vx))$ \\
   $\tilde{\mD} = \operatorname{Normalize}(\tilde{\mD})$ \\
   $\mD\mR = \mD\mR + \operatorname{RandomizedSVD}(\tilde{\mD} - \mD\mR)$ \\
   \For{$m \in \{1,\hdots M\}$}{
      $\mR_{m,m} = \|(\mD\mR)_m\|_F$ 
} 
}
 \caption{Dictionary Learning, Using ADMM with Large Number of Channels}
\end{algorithm}


\section{Conclusion}
In this chapter, I have derived a novel sparse coding algorithm for signals with a large number of channels. One of the steps in the iterative algorithm involves solving an inverse problem, but the optimization is constructed such that the representation of the inverse can be updated efficiently ($\mathcal{O}(K\mathbb{R}M^2)$, $\mathbb{R} << M$) when used within a dictionary-learning algorithm. This is an improvement to existing methods, whose inverse updates require cubic complexity $\mathcal{O}(KM^3)$.
% This is a figure in landscape orientation
%\begin{sidewaysfigure}
%\includegraphics[width=\textwidth]{figures/exampleFigure.png}
%\caption{This is another example Figure, rotated to landscape orientation.}
%\label{LandscapeFigure}
%\end{sidewaysfigure}
