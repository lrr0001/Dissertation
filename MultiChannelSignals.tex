\chapter{Learning Dictionaries for Multi-Channel Signals}

\section{Introduction}
When using a multi-layer dictionary model, the coefficients corresponding to a dictionary from one layer become the "signal" for the subsequent layer.  The number of channels for this "signal" is the number of dictionary filters from the previous layer.  Much of the literature on learning convolutional dictionaries is tailored to applications with signals that only have a small number of channels.  This chapter presents a novel method for learning convolutional dictionaries from and for multi-channel signals. 
\section{Dictionary Types}
There are many ways to construct a convolutional sparse representation of a multi-channel signal, but broadly the distinctions reduce down to if and how signal channels share dictionaries and coefficients, and if and how those non-shared entities interact across channels.

It is common in many applications for dictionary models to share dictionaries across channels, which requires the use multi-channel coefficients. If such models were used in a multi-layer dictionary model, the tensor rank would increase with each subsequent layer.

For this work, I focus instead on the multi-channel dictionary with shared coefficients. This structure matches that of convolutional neural networks, and the number of channels for a subsequent dictionary is the number of filters for the dictionary from the previous layer.
\section{Pursuit and Sparse Coding}
\label{section:sparse coding}
The dictionary model decomposes the signal $\vs_i$ into a dictionary $\mD$ (which generalizes to other signals) and the coefficients $\vx_i$ (which are specific to the signal $\vs_i$:

\begin{equation}
\vs_i \approx \mD\vx_i
\end{equation}

(Here the subscript $i$ specifies a particular signal and its corresponding coefficients.) A pursuit algorithm finds the coefficients $\vx_i$ corresponding to a particular signal $\vs_i$ for known dictionary $\mD$. If the number of dictionary atoms (columns) is larger than the dimension of the signal, then the number of unknowns is larger than the number of equations, and many solutions for $\vx_i$ represent $\vs_i$ equally well (at least in an L$2$ sense). Researchers and practitioners commonly either impose a sparsity constraint on the coefficients or add a coefficient L$1$ penalty to the objective function, which removes the ambiguity from the problem construction. When such a penalty or constraint is added, pursuit is sometimes called sparse coding. With the added coefficient L$1$ penalty, the pursuit optimization problem looks like this:

\begin{equation}\label{equation:sparse coding}
\vx_i = \arg\min_{\vx} \frac{1}{2}\|\vs_i - \mD\vx\|_2^2 + \lambda \|\vx\|_1
\end{equation}

where $\lambda$ is a hyperparameter greater than zero controlling how much the L$1$ norm of the coefficients is penalized. Researchers have proposed many ways to solve this problem. If the dictionary is convolutional and the number of channels is low, a standard approach is to use the Alternating direction Method of Multipliers (ADMM) algorithm.

\section{ADMM}
\label{section:ADMM}
ADMM is a convex-optimization algorithm used to solve the optimization problem:
\begin{equation} \label{equation:ADMM}
\begin{aligned}
\minimize_{\vx,\vy} & f(\vx) + g(\vy) \\
\text{subject to } & \mA\vx + \mB\vy + \vc = \vzero 
\end{aligned}
\end{equation}

where $f$ and $g$ are convex functions \cite{boyd2011distributed}. (I will address how to put the sparse coding problem in this form in the next section.)

The ADMM algorithm makes use of the augmented Lagrangian, a particular expression that has a saddle point at the solution to the constrained optimization problem:

\begin{equation}
\operatorname{L}_{\rho}(\vx,\vy,\vu) = f(\vx) + g(\vy) + \vu^H(\mA\vx + \mB\vy + \vc) + \frac{\rho}{2}\|\mA\vx + \mB\vy + \vc\|_2^2
\end{equation}

where $\rho$ is a hyperparameter greater than zero and $\vu$ is the dual variable for the constraints.

At the saddle-point solution, the augmented Lagrangian is at a minimum in respect to $\vx$ and $\vy$, but at a maximum in respect to $\vu$.

The ADMM algorithm is an iterative search for the saddle point of the augmented Lagrangian. Each iteration consists of a primal update for $\vx$, a primal update for $\vy$, and a dual update for $\vu$:

\begin{equation}
\vx^{(k + 1)} = \arg\min_{\vx} \operatorname{L}_{\rho}(\vx,\vy^{(k)},\vu^{(k)})
\end{equation}

\begin{equation}
\vy^{(k + 1)} = \arg\min_{\vy} \operatorname{L}_{\rho}(\vx^{(k + 1)},\vy,\vu^{(k)})
\end{equation}

\begin{equation}
\vu^{(k + 1)} = \vu^{(k)} + \rho(\mA\vx^{(k + 1)} + \mB\vy^{(k + 1)} + \vc)
\end{equation}

The primal updates serve to move towards the minimum of the augmented Lagrangian in respect to $\vx$ and $\vy$ with $\vu$ fixed, and the dual update fixes $\vx$ and $\vy$, and performs gradient ascent on $\vu$ with stepsize $\rho$. Under very mild assumptions, this process converges to the saddle point of the augmented Lagrangian, which matches the solution to the constrained optimization problem.

There are two common variations of the ADMM algorithm that this work will make use of.  The first is the scaled form, which comes from completing the square for the augmented lagrangian function:

\begin{equation}
\operatorname{L}_{\rho}(\vx,\vy,\vu) = f(\vx) + g(\vy) + \frac{\rho}{2}\|\mA\vx + \mB\vy + \vc + \frac{\vu}{\rho}\|_2^2 - \frac{1}{2\rho}\|\vu\|_2^2
\end{equation}

The term $-\frac{1}{2\rho}\|\vu\|_2^2$ can be ignored for the primal updates because it has no dependence on the primal variables. For this reason, it is sometimes more convenient to keep track of $\frac{\vu}{\rho}$ instead of $\vu$, since that is the form that appears in the augmented Lagrangian after completing the square.

\begin{equation}
\frac{\vu^{(k + 1)}}{\rho} = \frac{\vu^{(k)}}{\rho} + \mA\vx^{(k + 1)} + \mB\vy^{(k + 1)} + \vc
\end{equation}

This form is known as scaled ADMM.

The other common variation of ADMM updates the dual variable more frequently.
\begin{equation}
\vx^{(k + 1)} = \arg\min_{\vx} \operatorname{L}_{\rho}(\vx,\vy^{(k)},\vu^{(k)})
\end{equation}

\begin{equation}
\vu^{(k + \frac{1}{2})} = \vu^{(k)} + (\alpha - 1)\rho(\mA\vx^{(k + 1)} + \mB\vy^{(k)} + \vc)
\end{equation}

\begin{equation}
\vy^{(k + 1)} = \arg\min_{\vy} \operatorname{L}_{\rho}(\vx^{(k + 1)},\vy,\vu^{(k + \frac{1}{2})})
\end{equation}

\begin{equation}
\vu^{(k + 1)} = \vu^{(k + \frac{1}{2})} + \rho(\mA\vx^{(k + 1)} + \mB\vy^{(k + 1)} + \vc)
\end{equation}

When $\alpha > 1$, this is known as overrelaxation, and if $\alpha < 1$, this is known as under-relaxation.\footnote{I have chozen to notate over/under relaxation differently than what is standard, but the $\alpha$ is the same, and the notations are mathematically equivalent. The standard notation instead adds the term $-(1 - \alpha)(\mA\vx^{(k + 1)} + \mB\vy^{(k)} + \vc)$ to $\mA\vx^{(k + 1)}$ and substitutes that expression for $\mA\vx^{(k + 1)}$ in subsequent equations.} $\alpha$ is always chozen to be greater than zero. In some applications, researchers have found using over-relaxation converges faster than without overrelaxation \cite{eckstein1994parallel}, but optimal choice of $\alpha$ is problem-dependent \cite{nishihara2015general}.


\section{Applying ADMM to the Sparse Coding Problem}
\label{section:Applying ADMM}
Recall from section \ref{section:sparse coding}, equation \ref{equation:sparse coding} for sparse coding.

\begin{equation}
\vx_i = \arg\min_{\vx} \frac{1}{2}\|\vs_i - \mD\vx\|_2^2 + \lambda \|\vx\|_1
\end{equation}

This can be rewritten to match the ADMM form from equation \ref{equation:ADMM}:
\begin{equation} \label{equation:SC Opt Prob}
\begin{aligned}
\minimize_{\vx,\vy} & \frac{1}{2} \|\vs_i - \mD\vx\|_2^2 + \lambda\|\vy\|_1 \\
         \text{subject to } & \vy - \vx = \vzero
\end{aligned}
\end{equation}

Given sufficient iterations, $\vx$ and $\vy$ will both be close to the optimal, but they may not be equal. Either can be used an approximate solution to the sparse coding problem.

Computing the augmented Lagrangian of convex optimization problem in expression \ref{equation:SC Opt Prob} yields the following equation:

\begin{equation}
\operatorname{L}_{\rho}(\vx,\vy,\vu) = \frac{1}{2} \|\vs_i - \mD\vx\|_2^2 + \lambda\|\vy\|_1 + \frac{\rho}{2}\|\vy - \vx + \frac{\vu}{\rho}\|_2^2 - \frac{1}{2\rho}\|\vu\|_2^2
\end{equation}

Starting with the $\vx$-update:
\begin{equation}
\vx^{(k + 1)} = \arg\min_{\vx} \operatorname{L}_{\rho}(\vx,\vy^{(k)},\vu^{(k)})
\end{equation}

Since the minimim is desired, setting the gradient to zero will produce the solution.

\begin{equation}
\nabla_{\vx^{(k + 1)}} \operatorname{L}_{\rho}(\vx^{(k + 1)},\vy{(k)},\vu{(k)}) = \vzero
\end{equation}

\begin{equation}
\vzero = \mD^T\mD\vx^{(k + 1)} - \mD^T\vs_i + \rho\vx^{(k + 1)} - \rho(\vy^{(k)} + \frac{\vu^{(k)}}{\rho})
\end{equation}

\begin{equation}
(\rho\mId + \mD^T\mD)\vx^{(k + 1)} = \mD^T\vs_i + \rho(\vy^{(k)} + \frac{\vu^{(k)}}{\rho})
\end{equation}

\begin{equation}
\vx^{(k + 1)} = (\rho\mId + \mD^T\mD)^{-1}(\mD^T\vs_i + \rho(\vy^{(k)} + \frac{\vu^{(k)}}{\rho}))
\end{equation}

In subsection \ref{subsection:SC xupdate}, there is a discussion of the implications of this update equation, how to compute it for cases in which the signal has a low number of channels, and the challenges it poses for signals with many channels.

If using over-relaxation, there is a dual update:
\begin{equation}
\frac{\vu^{(k + \frac{1}{2})}}{\rho} = \frac{\vu^{(k)}}{\rho} + (\alpha - 1)(\vy^{(k)} - \vx^{(k + 1)})
\end{equation}

Moving on to the $\vy$-update:
\begin{equation}
\vy^{(k + 1)} = \arg\min_{\vy} \operatorname{L}_{\rho}(\vx^{(k + 1)},\vy,\vu^{(k + \frac{1}{2})})
\end{equation}

Excluding the terms that don't include $\vy$, I have
\begin{equation}
\vy^{(k + 1)} = \arg\min_{\vy} \lambda\|\vy\|_1 + \frac{\rho}{2}\|\vy - \vx^{(k + 1)} + \frac{\vu^{(k + \frac{1}{2})}}{\rho}\|_2^2
\end{equation}

This is a well-known problem, whose solution is
\begin{equation}
\vy^{(k + 1)} = \operatorname{S}_{\frac{\lambda}{\rho}}(\vx^{(k + 1)} - \frac{\vu^{(k + \frac{1}{2})}}{\rho})
\end{equation}
where $\operatorname{S}$ is the shrinkage operator:
\begin{equation}
\operatorname{S}_{b}(x) = \begin{cases} x - b & x > b \\ 0 & -b < x < b \\ x + b & x < - b \end{cases}
\end{equation}
In the case of a vector, matrix, or tensor input, the shrinkage operator is applied element by element.

Finally, the last update equation for the dual variable:
\begin{equation}
\frac{\vu^{(k + 1)}}{\rho} = \frac{\vu^{(k + \frac{1}{2})}}{\rho} + \vy^{(k + 1)} - \vx^{(k + 1)}
\end{equation}

\subsection{Exploiting Dictionary Structure for the Inverse Problem}
\label{subsection:SC xupdate}
Returning to the $\vx$ update:
\begin{equation}
\vx^{(k + 1)} = (\rho\mId + \mD^T\mD)^{-1}(\mD^T\vs_i + \rho(\vy^{(k)} + \frac{\vu^{(k)}}{\rho}))
\end{equation}

For problems using a dictionary with convolutional structure, this inverse for the convolutional sparse coding problem is very structured. Exploiting this structure is important for efficient computation, because the matrix $\rho\mId + \mD^T\mD$ is a large matrix.

Writing $\mD$ in a block structure, I have

\begin{equation}
\mD = \begin{bmatrix}
\mD_{1,1}, & \hdots, & \mD_{1,M}\\
\vdots & \ddots & \vdots\\
\mD_{C,1}, & \hdots, & \mD_{C,M}
\end{bmatrix}
\end{equation}

where $\mD_{c,m}$ is a toplitz matrix capturing channel $c$ of the $m$th filter of the dictionary. Toplitz matrices are diagonalizable with Fourier eigenvectors:

\begin{equation}
\mD = \begin{bmatrix}
\F^{-1}\hat{\mD}_{1,1}\F, & \hdots, & \F^{-1}\hat{\mD}_{1,M}\F\\
\vdots & \ddots & \vdots\\
\F^{-1}\hat{\mD}_{C,1}\F, & \hdots, & \F^{-1}\hat{\mD}_{C,M}\F
\end{bmatrix}
\end{equation}

where $\hat{\mD}_{c,m}$ is a diagonal matrix whose elements are the discrete Fourier transform (FFT) of channel $c$ of the $m$th dictionary filter.

This sparsely banded structure is a useful form in analyzing the structure of the inverse problem:
\begin{equation}
(\rho \mId + \mD^T\mD)^{-1} = \F^{-1}(\rho\mId + \hat{\mD}^H\hat{\mD})^{-1}\F
\end{equation}

where
\begin{equation}
\hat{\mD} = \begin{bmatrix}
\hat{\mD}_{1,1}, & \hdots, & \hat{\mD}_{1,M}\\
\vdots & \ddots & \vdots\\
\hat{\mD}_{C,1}, & \hdots, & \hat{\mD}_{C,M}
\end{bmatrix}
\end{equation}

and in a slight abuse of notation, $\F$ computes the FFT separately on the coefficients for each filter.  In \cite{Bristow2013fast}, Bristow et al. observe the matrix $\rho\mId + \hat{\mD}^H\hat{\mD}$ is sparsely banded, so the inverse can be broken down into much smaller inverse problems, and one only needs to compute the inverse of an $M \times M$ matrix for every pixel in the image. ($\rho\mId + \hat{\mD}^H\hat{\mD}$ is an $M \times M$ block matrix, whose blocks are diagonal.  Each submatrix collects one element from the diagonal of each of the blocks.)

Furthermore, the maximum rank of these submatrices is $C$, so if $C$ is small, these inverses can be computed even more efficiently using the Woodbury matrix identity \cite{vsorel2016fast} \cite{heide2015fast} \cite{wohlberg2015efficient}.

According to the Woodbury matrix identity, for any invertible matrix $\mU$ and any matrix $\mV$:
\begin{equation}
(\mU + \mV^H\mV)^{-1} = \mU^{-1} - \mU^{-1}\mV^H(\mId + \mV\mU^{-1}\mV^H)^{-1}\mV\mU^{-1}
\end{equation}

So,
\begin{equation}
(\rho\mId + \hat{\mD}^H\hat{\mD})^{-1} = \frac{1}{\rho}\mId - \frac{1}{\rho}\hat{\mD}^H(\rho\mId + \hat{\mD}\hat{\mD}^H)^{-1}\hat{\mD}
\end{equation}

This means that instead of computing the inverse of an $M \times M$ matrix for every pixel in the image, one could instead choose to compute the inverse of a $C \times C$ matrix for each pixel in the image.\footnote{Generally, Cholesky or LDLT decomposition would be preferable to explicitly computing the inverse, and the efficiency gains due to the Woodbury matrix identity are relevant regardless of the chosen representation.}

\section{Literature Review}
In applying ADMM to the convolutional sparse coding problem, \cite{vsorel2016fast} \cite{heide2015fast} \cite{wohlberg2015efficient} exploit the low-rank structure of the inverse problem in the $\vx$ update for efficient computation. Unfortunately, this relies on the number of channels being small. This section reviews sparse coding methods that avoid or simplify this challenging inverse problem.

In \cite{chodosh2018deep}\cite{murdock2018deep}, the authors use the ADMM algorithm for sparse coding. They observe that if the dictionary is a tight frame, that is, $\mD\mD^T = \mId$, then the inverse can be simplied without using the frequency representation.

\begin{equation}
(\rho\mId + \mD^T\mD)^{-1} = \frac{1}{\rho}\mId - \frac{1}{\rho(\rho + 1)}\mD^T\mD
\end{equation}

This produces an update equation for $\vx$ that avoids computing inverses altogether.

\begin{equation}
\vx^{(k + 1)} = \frac{1}{\rho + 1}\mD^T\vs + (\mId - \frac{1}{\rho + 1}\mD^T\mD)(\vz^{(k)} - \frac{\vgamma^{(k)}}{\rho})
\end{equation}

In their work, they use the equations built on the assumption that the dictionary is a tight frame, but develop no mechanism to ensure that their assumption is accurate. Thus, ultimately $\frac{1}{\rho}\mId - \frac{1}{\rho(\rho + 1)}\mD^T\mD$ serves as an approximation to $(\rho\mId + \mD^T\mD)^{-1}$. The dictionaries they learn are not tight frames.

Other works avoid the ADMM algorithm entirely.

The iterative shrinkage thresholding algorithm (ISTA) is an iterative algorithm that minimizes the sum of two convex functions $f$ and $g$. $f$ is required to be smooth. It is helpful for $f$ to be easily differentiable and $g$ to have a simple proximal operator.

\begin{equation}
\prox_g(\vmu) = \arg \min_{\vnu} \frac{1}{2}\|\vnu - \vmu\|_2^2 + g(\vnu)
\end{equation}

Then, ISTA has the following update equation, where the constant $L$ constrols step size.

\begin{equation}
\vx^{(k + 1)} = \prox_g(\vx^{(k)} - \frac{1}{L}\nabla_{\vx} f(\vx^{(k)}))
\end{equation}

FISTA is similar to ISTA, but adds momentum \cite{beck2009fast}.
\begin{equation}
\vz^{(k + 1)} = \prox_g(\vx^{(k)} - \frac{1}{L}\nabla_{\vx}f(x^{(k)}))
\end{equation}
\begin{equation}
t^{(k + 1)} = \frac{1}{2}(1 + \sqrt{1 + 4(t^{(k)})^2})
\end{equation}
\begin{equation}
\vx^{(k + 1)} = \vz^{(k + 1)} + \frac{t^{(k)} - 1}{t^{(k + 1)}}(\vz^{(k + 1)} - \vx^{(k)})
\end{equation}

Applying FISTA to the sparse coding problem, $\frac{1}{2}\|\vs - \mD\vx\|_2^2$ is straightforward to differentiate and $\lambda\|\vx\|_1$ has a simple proximal operator.

\begin{equation}
\nabla_{\vx}(\frac{1}{2}\|\vs - \mD\vx\|_2^2) = \mD^T\mD\vx - \mD^T\vs
\end{equation}

\begin{equation}
\prox_{\lambda\|\cdot\|_1}(\cdot) = \operatorname{S}_{\lambda}
\end{equation}

So, the FISTA equations for convolutional basis pursuit are the following:
\begin{equation}
\vz^{(k + 1)} = \operatorname{S}_{\lambda}(\vx^{(k)} - \frac{1}{L}\mD^T(\mD\vx^{(k)} - \vs))
\end{equation}
\begin{equation}
t^{(k + 1)} = \frac{1}{2}(1 + \sqrt{1 + 4(t^{(k)})^2})
\end{equation}
\begin{equation}
\vx^{(k + 1)} = \vz^{(k + 1)} + \frac{t^{(k)} - 1}{t^{(k + 1)}}(\vz^{(k + 1)} - \vx^{(k)})
\end{equation}

In \cite{wohlberg2015efficient}, Wohlberg compares FISTA to ADMM on a sparse coding task and finds FISTA converges much slower than ADMM. However, the comparison is made on signals with few channels, so ADMM is able to exploit the structure of $\mD$ for efficient $\vx$ updates.

In a recent work \cite{chodosh2020use}, Chodosh and Lucey use updates prox-linear updates using more general convex solver methods detailed in \cite{xu2013block}.

The updates come from the formula:
\begin{equation}
\vx^{(k + 1)} = \arg\min_{\vx} (\nabla f(\bar{\vx}^{(k)}))^T(\vx - \bar{\vx}^{(k)}) + \frac{L}{2}\|\vx - \bar{\vx}^{(k)}\|_2^2 + \lambda\|\vx\|_1
\end{equation}

where $\bar{\vx}^{(k)} = \vx^{(k)} + \omega_k(\vx^{(k)} - \vx^{(k - 1)})$ and $\omega_k$ is a momentum factor.

This yields the update equation\footnote{In their paper, they add a non-negativity constraint and allow different $\lambda$ for the coefficients of each filter (and possibly spatially varied as well). They also are constucting the equations specifically for a multi-layer network. I simplified their equations to illistrate how their approach relates to the FISTA algorithm.}:
\begin{equation}
\vx^{(k + 1)} = \operatorname{S}_{\frac{\lambda}{L}}(\bar{\vx}^{(k)} + \mD^T(\vs - \mD\bar{\vx}))
\end{equation}

While neither Chodosh and Lucey nor the work they cite mentions FISTA, the resemblance is very close. There are two distinctions:
\begin{enumerate}
\item Momentum is computed slightly differently: to match FISTA, the prox-linear updates would need to use $\vx^{(k)} - \bar{\vx}^{(k - 1)}$ for momentum. Instead, they use $\vx^{(k)} - \vx^{(k - 1)}$.
\item The prox-linear approach scales the momentum steps differently.
\end{enumerate}

Given these similarities, it is likely the performance between the two methods is similar.

\section{ADMM with Low-Rank Updates}
In this section, I present a novel approach to sparse coding for signals with a large number of channels. The approach uses the ADMM algorithm described in section \ref{section:ADMM} and will share many similarities to the standard ADMM sparse coding approach described in section \ref{section:Applying ADMM} for signals with few channels.
\section{Conclusion}

% This is a figure in landscape orientation
\begin{sidewaysfigure}
\includegraphics[width=\textwidth]{figures/exampleFigure.png}
\caption{This is another example Figure, rotated to landscape orientation.}
\label{LandscapeFigure}
\end{sidewaysfigure}
