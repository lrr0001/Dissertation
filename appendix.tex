\begin{appendices}

%Some Table of Contents entry formatting
\addtocontents{toc}{\protect\renewcommand{\protect\cftchappresnum}{\appendixname\space}}
\addtocontents{toc}{\protect\renewcommand{\protect\cftchapnumwidth}{6em}}

%Begin individual appendices, separated as chapters

\chapter{Hermitian Rank-1 Updates for the Cholesky Decomposition} \label{chapter:Cholesky}
These derivations are based on the work of \cite{krause2015more}, but modified to handle complex numbers.


Let $\mA^{(n)} = \mL\mL^H$ be a Hermitian, positive definite matrix of dimension $C \times C$ and $\mL$ be a lower triangular matrix. Then, the diagonal of $\mL$ is positive and real.

\begin{equation}
\mA^{(n)} = \begin{bmatrix} a_{1,1} & \hdots & a_{C,1}^{*} \\
                      \vdots & \ddots & \vdots \\
                      a_{C,1} & \hdots & a_{C,C}
      \end{bmatrix}
\end{equation}

\begin{equation}
\mL = \begin{bmatrix} \ell_{1,1} & 0      & \hdots & 0 \\
                      \ell_{2,1} & \ddots &        & \vdots \\
                      \vdots     &        &  \ddots &  0     \\
                      \ell_{C,1}     & \hdots &   & \ell_{C,C}
      \end{bmatrix}
\end{equation}

Dividing $\mA^{(n)}$ into blocks:
\begin{equation}
\mA^{(n)} = \begin{bmatrix}\ell_{1,1} & \vzero^T \\ \vl_{2,1} & \mL_{2,2} \end{bmatrix} \begin{bmatrix} \ell_{1,1}^{*} & \vl_{2,1}^H \\ \vzero & \mL_{2,2}^H \end{bmatrix} 
\end{equation}

\begin{equation}
\mA^{(n)} = \begin{bmatrix} \ell_{1,1}^2 & \ell_{1,1}\vl_{2,1}^H \\ \ell_{1,1}\vl_{2,1} & \mL_{2,2}\mL_{2,2}^H + \vl_{2,1}\vl_{2,1}^H \end{bmatrix}
\end{equation}

Consider the rank-$1$ update:\footnote{If $\lambda$ is negative, the result is not guaranteed to be positive definite. In general, updates to inverse representations with a negative $\lambda$ (sometimes called "downdates") are less numerically stable, even if the resulting matrix is still positive definite.}
\begin{equation}
\mA^{(n + 1)} = \mA^{(n)} + \lambda\vv\vv^H
\end{equation}

where
\begin{equation}
\vv = \begin{bmatrix} v_1 \\ \vv_2 \end{bmatrix}
\end{equation}

\begin{equation}
\mA^{(n + 1)} = \begin{bmatrix} \ell_{1,1}^2 + \lambda v_1 v_1^{*} & \ell_{1,1}\vl_{2,1}^H + \lambda v_1\vv_2^H \\ \ell_{1,1}\vl_{2,1} + \lambda v_1^{*}\vv_2 & \mL_{2,2}\mL_{2,2}^H + \vl_{2,1}\vl_{2,1}^H + \lambda\vv_2\vv_2^H \end{bmatrix}
\end{equation}

Let $\mA^{(n + 1)} = \mM\mM^H$, where $\mM$ is a lower triangular matrix. Then, the diagonal of $\mM$ is positive and real.

\begin{equation}
\mA^{(n + 1)} = \begin{bmatrix} m_{1,1}^2 & m_{1,1}\vm_{2,1}^H \\ m_{1,1}\vm_{2,1} & \mM_{2,2}\mM_{2,2}^H + \vm_{2,1}\vm_{2,1}^H \end{bmatrix}
\end{equation}

Therefore,
\begin{equation}
m_{1,1}^2 = \ell_{1,1}^2 + \lambda v_1 v_1^{*}
\end{equation}
\begin{equation}
m_{1,1}\vm_{2,1} = \ell_{1,1}\vl_{2,1} + \lambda v_1^{*}\vv_2
\end{equation}
\begin{equation}
\mM_{2,2}\mM_{2,2}^H + \vm_{2,1}\vm_{2,1}^H = \mL_{2,2}\mL_{2,2}^H + \vl_{2,1}\vl_{2,1}^H + \lambda\vv_2\vv_2^H
\end{equation}

Solving for the first column of $\mM$:
\begin{equation} \label{equation:m11}
m_{1,1} = \sqrt{\ell_{1,1}^2 + \lambda v_1 v_1^{*}}
\end{equation}
\begin{equation}
\vm_{2,1} = \frac{\ell_{1,1}\vl_{2,1} + \lambda v_1^{*}\vv_2}{m_{1,1}}
\end{equation}
Finally,
\begin{equation}
\mM_{2,2}\mM_{2,2}^H = \mL_{2,2}\mL_{2,2}^H + \vl_{2,1}\vl_{2,1}^H + \lambda\vv_2\vv_2^H - \vm_{2,1}\vm_{2,1}^H
\end{equation}

\begin{equation}
\vm_{2,1}\vm_{2,1}^H = \frac{1}{m_{1,1}^2}\left(\ell_{1,1}^2\vl_{2,1}\vl_{2,1}^H + \lambda\ell_{1,1}v_1\vl_{2,1}\vv_2^H + \lambda\ell_{1,1}v_1^{*}\vv_2\vl_{2,1}^H + \lambda^2 v_1 v_1^{*} \vv_2\vv_2^H\right)
\end{equation}

\begin{multline}
\mM_{2,2}\mM_{2,2}^H = \mL_{2,2}\mL_{2,2}^H + \frac{m_{1,1}^2 - \ell_{1,1}^2}{m_{1,1}^2}\vl_{2,1}\vl_{2,1}^H + \frac{\lambda(m_{1,1}^2 - \lambda v_1 v_1^{*})}{m_{1,1}^2}\vv_2\vv_2^H \\- \left(\frac{\lambda\ell_{1,1}v_1}{m_{1,1}^2}\vl_{2,1}\vv_2^H + \frac{\lambda\ell_{1,1}v_1^{*}}{m_{1,1}}\vv_2\vl_{2,1}^H\right)
\end{multline}

The expressions $m_{1,1}^2 - \lambda v_1 v_1^{*}$ and $m_{1,1}^2 - \ell_{1,1}^2$ can be simplified using equation \ref{equation:m11}.
\begin{equation}
\mM_{2,2}\mM_{2,2}^H = \mL_{2,2}\mL_{2,2}^H + \frac{\lambda v_1 v_1^{*} }{m_{1,1}^2}\vl_{2,1}\vl_{2,1}^H + \frac{\lambda \ell_{1,1}^2}{m_{1,1}^2}\vv_2\vv_2^H - \frac{\lambda\ell_{1,1}v_1}{m_{1,1}}\vl_{2,1}\vv_2^H - \frac{\lambda\ell_{1,1}v_1^{*}}{m_{1,1}^2}\vv_2\vl_{2,1}^H
\end{equation}

Factoring out $\frac{\lambda}{m_{1,1}^2}$:
\begin{equation}
\mM_{2,2}\mM_{2,2}^H = \mL_{2,2}\mL_{2,2}^H + \frac{\lambda}{m_{1,1}^2}\left( v_1 v_1^{*} \vl_{2,1}\vl_{2,1}^H + \ell_{1,1}^2\vv_2\vv_2^H - \ell_{1,1}v_1\vl_{2,1}\vv_2^H - \ell_{1,1}v_1^{*}\vv_2\vl_{2,1}^H\right)
\end{equation}

Note the factorization:
\begin{equation}
(\ell_{1,1}\vv_2 - v_1\vl_{2,1})(\ell_{1,1}\vv_2 - v_1\vl_{2,1})^H = \ell_{1,1}^2\vv_2\vv_2^H - \ell_{1,1}v_1^{*}\vl_{2,1}^H - \ell_{1,1}v_1\vl_{2,1}\vv_2 + v_1 v_1^{*}\vl_{2,1}\vl_{2,1}^H
\end{equation}
Therefore,
\begin{equation}
\mM_{2,2}\mM_{2,2}^H = \mL_{2,2}\mL_{2,2}^H + \frac{\lambda}{m_{1,1}^2}\left(\ell_{1,1}\vv_2 - v_1\vl_{2,1}\right)\left(\ell_{1,1}\vv_2 - v_1\vl_{2,1}\right)^H
\end{equation}
$\mL_{2,2}\mL_{2,2}^H$ is a $ (C - 1) \times (C - 1)$ Hermitian, positive definite matrix and $\frac{\lambda}{m_{1,1}^2}(\ell_{1,1}\vv_2 - v_1\vl_{2,1})(\ell_{1,1}\vv_2 - v_1\vl_{2,1})^H$ is a rank-$1$ Hermitian update, so the process can be repeated on subsequent columns of $\mL$ until the entire Cholesky decomposition has been updated. Each column update is computed in linear time, so the entire update can be computed in quadratic time.

While the order of complexity cannot be further reduced, there are changes that can be made to decrease precision error. Factoring out $\ell_{1,1}$:
\begin{equation}
\mM_{2,2}\mM_{2,2}^H = \mL_{2,2}\mL_{2,2}^H + \frac{\lambda\ell_{1,1}^2}{m_{1,1}^2}\left(\vv_2 - \frac{v_1}{\ell_{1,1}}\vl_{2,1}\right)\left(\vv_2 - \frac{v_1}{\ell_{1,1}}\vl_{2,1}\right)^H
\end{equation}

Rather than directly updating $\lambda$ before moving on to the next column, better precision can be achieved by updating a divisor instead.  Looking at the fraction $\frac{\ell_{1,1}^2}{m_{1,1}^2}$:
\begin{equation}
\frac{\ell_{1,1}^2}{m_{1,1}^2} = \frac{\ell_{1,1}^2}{\ell_{1,1}^2 + \lambda v_1 v_1^{*}}
\end{equation}

\begin{equation}
\frac{\ell_{1,1}^2}{m_{1,1}^2} = \frac{1}{1 + \frac{\lambda v_1 v_1^{*}}{\ell_{1,1}^2}}
\end{equation}

Therefore,
\begin{equation}
\mM_{2,2}\mM_{2,2}^H = \mL_{2,2}\mL_{2,2}^H + \frac{\lambda}{1 + \frac{\lambda v_1 v_1^{*}}{\ell_{1,1}^2}}(\vv_2 - \frac{v_1}{\ell_{1,1}}\vl_{2,1})(\vv_2 - \frac{v_1}{\ell_{1,1}}\vl_{2,1})^H
\end{equation}

Let
\begin{equation}
\vomega = \vv_2 - \frac{v_1}{\ell_{1,1}}\vl_{2,1}
\end{equation}

So,
\begin{equation}
\mM_{2,2}\mM_{2,2}^H = \mL_{2,2}\mL_{2,2}^H + \frac{\lambda}{1 + \frac{\lambda v_1 v_1^{*}}{\ell_{1,1}^2}}\vomega\vomega^H
\end{equation}

Previously, $\vm_{2,1}$ was written in terms of $\vl_{2,1}$ and $\vv$. It can instead be written in terms of $\vl_{2,1}$ and $\vomega$. Recall that
\begin{equation}
\vm_{2,1} = \frac{\ell_{1,1}\vl_{2,1} + \lambda v_1^{*}\vv_2}{m_{1,1}}
\end{equation}

Substituting for $\vv_2$:
\begin{equation}
\vm_{2,1} = \frac{\ell_{1,1}\vl_{2,1} + \lambda v_1^{*}(\vomega + \frac{v_1}{\ell_{1,1}}\vl_{2,1})}{m_{1,1}}
\end{equation}

Combining the $\vl_{2,1}$ terms:
\begin{equation}
\vm_{2,1} = \frac{(\ell_{1,1} +  \frac{\lambda v_1 v_1^{*}}{\ell_{1,1}}) \vl_{2,1} + \lambda v_1^{*}\vomega}{m_{1,1}}
\end{equation}

Factoring out $\frac{1}{\ell_{1,1}}$ produces the expression for $m_{1,1}^2$.
\begin{equation}
\vm_{2,1} = \frac{\frac{1}{\ell_{1,1}}(\ell_{1,1}^2 +  \lambda v_1 v_1^{*}) \vl_{2,1} + \lambda v_1^{*}\vomega}{m_{1,1}}
\end{equation}
\begin{equation}
\vm_{2,1} = \frac{\frac{m_{1,1}^2}{\ell_{1,1}} \vl_{2,1} + \lambda v_1^{*}\vomega}{m_{1,1}}
\end{equation}

\begin{equation}
\vm_{2,1} = \frac{m_{1,1}}{\ell_{1,1}} \vl_{2,1} + \frac{\lambda v_1^{*}}{m_{1,1}}\vomega
\end{equation}

So, to summarize:
\begin{equation}
m_{1,1} = \sqrt{\ell_{1,1}^2 + \lambda |v_1|^2}
\end{equation}

\begin{equation}
\vomega = \vv_2 - \frac{v_1}{\ell_{1,1}}\vl_{2,1}
\end{equation}

\begin{equation}
\vm_{2,1} = \frac{m_{1,1}}{\ell_{1,1}} \vl_{2,1} + \frac{\lambda v_1^{*}}{m_{1,1}}\vomega
\end{equation}

\begin{equation}
\mM_{2,2}\mM_{2,2}^H = \mL_{2,2}\mL_{2,2}^H + \frac{\lambda}{1 + \frac{\lambda |v_1|^2}{\ell_{1,1}^2}}\vomega\vomega^H
\end{equation}
The rank-$1$ Hermitian update to a Cholesky decomposition can be computed in quadratic time. The algorithm can be rewritten to be computed in place.

\begin{algorithm}[H]
\SetAlgoLined
 %initialization:\\
   $\vomega \leftarrow \vv$ \\
   $\alpha \leftarrow 1$ \\
  \For{$i \in \{1,\hdots,C\}$}{
     $\eta \leftarrow \alpha\ell_{i,i}^2 + \lambda |\omega_i|^2$ \\
     $m_{i,i} \leftarrow \sqrt{\ell_{i,i}^2 + \frac{\lambda|\omega_i|^2}{\alpha}}$ \\
     \For{$j \in \{i + 1,\hdots, C\}$}{
        $\omega_j \leftarrow \omega_j - \frac{\ell_{j,i}\omega_i}{\ell_{i,i}} $ \\
        $m_{j,i} \leftarrow \frac{m_{i,i}\ell_{j,i}}{\ell_{i,i}} + \frac{\lambda m_{i,i}\omega_j\omega_i^{*}}{\eta}$
}
     $\alpha \leftarrow \alpha + \frac{\lambda|\omega_i|^2}{\tau^2}$
}
 \caption{Cholesky Decomposition Hermitian Rank-One Update}
\end{algorithm} 




\begin{algorithm}[H]
\SetAlgoLined
 %initialization:\\
   $\vomega \leftarrow \vv$ \\
   $\alpha \leftarrow 1$ \\
  \For{$i \in \{1,\hdots,C\}$}{
     $\eta \leftarrow \alpha\ell_{i,i}^2 + \lambda |\omega_i|^2$ \\
     $\tau \leftarrow \ell_{i,i}$ \\
     $\ell_{i,i} \leftarrow \sqrt{\ell_{i,i}^2 + \frac{\lambda|\omega_i|^2}{\alpha}}$ \\
     \For{$j \in \{i + 1,\hdots, C\}$}{
        $\omega_j \leftarrow \omega_j - \frac{\ell_{j,i}\omega_i}{\tau} $ \\
        $\ell_{j,i} \leftarrow \frac{\ell_{i,i}\ell_{j,i}}{\tau} + \frac{\lambda\ell_{i,i}\omega_j\omega_i^{*}}{\eta}$
}
     $\alpha \leftarrow \alpha + \frac{\lambda|\omega_i|^2}{\tau^2}$
}
 \caption{Cholesky Decomposition Hermitian Rank-One Update (In Place)}
\end{algorithm} 




% ************ APPENDIx B ***********
\chapter{Rank-2 Eigendecomposition Edge Cases} \label{chapter:eigenedge}
\begin{equation}
\mA = \begin{bmatrix}  \vv  &  \mD^H\vu \end{bmatrix}
\end{equation}
\begin{equation}
\mB = \begin{bmatrix} \mD^H\vu & \vv\end{bmatrix}^H
\end{equation}
\section{Less than 2 Independent Eigenvectors}




The matrix $\mA\mB$ is a Hermitian $M \times M$ matrix, so it has $M$ real eigenvalues and $M$ independent eigenvectors which can be chosen to be orthogonal. Therefore, $\mB\mA$ has $2$ independent eigenvectors if $\mB$ is rank $2$.  There are three cases that can cause $\mB$ to have a rank less than $2$.


\begin{enumerate}
\item
\begin{equation}
\mD^H\vu = \vzero
\end{equation}
\item

\begin{equation}
\vv = \vzero
\end{equation}
\item
\begin{equation}
\mD^H\vu = \alpha \vv
\end{equation}
\end{enumerate}
where $\alpha$ is a scalar.

In the first $2$ cases, the matrix $\mB\mA$ has one independent eigenvector. The first case implies that only the Hermitian update is nonzero.  The second case implies that the entire update is zero. (The eigenvalues are zero, so as long as the normalization of eigenvectors is handled with care, it is not necessary to check for these cases in code.)

In the third case, the diagonalization of $\mA\mB$ can be determined directly without using the $2 \times 2$ matrix $\mB\mA$.

\begin{equation}
\mA\mB = 2
\real(\alpha)\vv\vv^H
\end{equation}

\begin{equation}
\tau = 2
\real(\alpha) \|\vv\|_2^2
\end{equation}

\begin{equation}
\vomega = \frac{\vv}{\|\vv\|_2}
\end{equation}

The rest of the eigenvalues are zero.  (The corresponding $2 \times 2$ matrix $\mB\mA$ shares the same nonzero eigenvalue. The eigenvector that is lost has an eigenvalue of zero, so like in the other $2$ cases, it is not necessary to check for this case in code.)

\section{Eigenvalues are Not Distinct}

If the pair of eigenvalues are the same, then all nonzero vectors are eigenvectors of $\mB\mA$. However, it is necessary for a diagonalization expansion with only Hermitian terms that the eigenvectors of $\mA\mB$ are chosen to be orthogonal, which can be found using a Gram-Schmidt process. This case is not just a theoretical concern; it is necessary to check for this in code.



% ************ APPENDIx C ***********
\chapter{Differentiating the Inverse Function} \label{chapter:Differentiating the Inverse Function}
Appendix \ref{chapter:Differentiating the Inverse Function} considers the equation
\begin{equation}
f_{\hat{\mD}}(\vb) = \left(\rho\mId + \hat{\mD}^H\hat{\mD}\right)^{-1}\vb \label{equation:Standard Inversion Function}
\end{equation}
and derives the mathematical equations necessary to apply the chain rule to composite functions containing function $f_{\hat{\mD}}$, treating $\rho$ as a real constant.

\section{Chain Rule} \label{section:Chain Rule}
When differentiating composite functions, the chain rule equation is really useful.
\begin{equation}
\frac{\partial g(f(x))}{\partial x} = \frac{\partial g(f(x))}{\partial f(x)}\frac{\partial f(x)}{\partial x}
\end{equation}
If $f_a(x)$ and $g_a(y)$ both depend on real scalar variable $a$, then
\begin{equation}
\frac{\partial g_a(f_a(x))}{\partial a} = \frac{\partial g_a(y)}{\partial a} + \frac{\partial g_a(f_a(x))}{\partial f_a(x)}\frac{\partial f_a(x)}{\partial a}
\end{equation}
substituting in $y = f_a(x)$ after differentiation.
Applying this inductively, the partial derivatives of a composite function can be derived using expressions for the partial derivatives of each of the component functions. So, if $f_a(x)$ is a component function, the partial derivatives that need to be calculated are $\frac{\partial f_a(x)}{\partial a}$ and $\frac{\partial f_a(x)}{\partial x}$.


\subsection{Matrix Calculus}
There are competing standards for matrix calculus notation. This dissertation will default to the numerator layout:
\begin{equation}
\frac{d\vy}{d\vx} = \begin{bmatrix}
\frac{dy_1}{dx_1} & \hdots & \frac{dy_1}{dx_n} \\ 
\vdots & \ddots & \vdots \\
\frac{dy_m}{dx_1} & \hdots & \frac{dy_n}{dx_n}
\end{bmatrix}
\end{equation}

The corresponding chain rule equations are
\begin{equation}
\frac{\partial g(f(\vb))}{\partial \vb} = \frac{\partial g(\vy)}{\partial \vy}\frac{\partial f(\vb)}{\partial \vb}
\end{equation}
\begin{equation}
\frac{\partial g_a(f_a(\vb))}{\partial a} = \frac{\partial g_a(\vy)}{\partial a} + \frac{\partial g_a(\vy)}{\partial \vy}\frac{\partial f_a(\vb)}{\partial a}
\end{equation}
So, if $f_a(\vb)$ is a component function, its partial derivatives that need to be calculated are $\frac{\partial f_a(\vb)}{\partial a}$ and $\frac{\partial f_a(\vb)}{\partial \vb}$.

Even for functions with branching, the necessary partial derivatives are the same. In the branching case, derivatives are aggregated just as they were with the shared dependence on scalar variable $a$. For example,
\begin{equation}
\frac{\partial g(f_1(\vb),f_2(\vb))}{\partial \vb} = \frac{\partial g(f_1(\vb),f_2(\vb))}{\partial f_1(\vb)}\frac{\partial f_1(\vb)}{\partial \vb} + \frac{\partial g(f_1(\vb),f_2(\vb))}{\partial f_2(\vb)}\frac{\partial f_2(\vb)}{\partial \vb} 
\end{equation}

If $\loss$ is a real-valued scalar function that depends on $\vb \in \R^M$, then\footnote{The transpose is necessary because the derivatives adhere to numerator layout.}
\begin{equation}
\nabla_{\vb} \loss = \left(\frac{\partial \loss}{\partial \vb}\right)^T
\end{equation}
If $\vb$ affects $\loss$ through multiple branches, the gradient is the sum of multiple terms. I will use a superscript to specify one of the terms by its branch:
\begin{equation}
\nabla^{(\vb \rightarrow f_i(\vb))}_{\vb} \loss = \left(\frac{\partial \loss}{\partial f_i(\vb)}\frac{\partial f_i(\vb)}{\partial \vb}\right)^T
\end{equation}
\begin{equation}
\nabla_{\vb} \loss = \sum_i \nabla^{(\vb \rightarrow f_i(\vb))}_{\vb} \loss
\end{equation}

\subsection{Complex Numbers}
The chain rule can be applied for functions of complex variables if the functions are analytic. However, for a non-analytic function $f$, the limit $\lim_{\Delta z \rightarrow 0} \frac{f(z + \Delta z) - f(z)}{\Delta z}$ does not exist. Since all non-constant functions onto reals are nonanalytic, the lack of derivative is a problem. Fortunately, for functions onto reals, a chain-rule like property exists for gradients. For non-analytic function $f: \Z^M \rightarrow \R$, the gradient is simply\footnote{The partial derivatives in this definition are not guaranteed to exist. For those cases, this derivative cannot be calculated. For the rest of this part of the appendix, assume that the non-analytic functions are chosen such that the partial derivatives exist.}
\begin{equation} \label{equation:Gradient Definition}
\nabla_{\vz} f(\vz) = \left(\frac{\partial f(\vz)}{\partial \real(\vz)} - j\frac{\partial f(\vz)}{\partial \imag(\vz)}\right)^H
\end{equation}

If a non-analytic, real-valued function $g: \Z^N \rightarrow \R$ is paired with analytic function $f: \Z^M \rightarrow \Z^N$, then the chain rule can safely be applied for composite function $g \circ f$, treating the Hermitian transpose of the gradient as a derivative.
\begin{equation} \label{equation:Complex Chain Rule}
\left(\nabla_{\vz} g(f(\vz))\right)^H = \left( \nabla_{f(\vz)} g(f(\vz))\right)^H \frac{\partial f(\vz)}{\partial \vz}
\end{equation}

To see that this is true, the key is to split the complex numbers into real and complex components, and apply the chain rule:
\begin{equation}
\frac{\partial g(f(\vz))}{\partial \real(\vz)} = \frac{\partial g(f(\vz))}{\partial \real(f(\vz))}\frac{\partial \real(f(\vz))}{\partial \real(\vz)} + \frac{\partial g(f(\vz))}{\partial \imag(f(\vz))}\frac{\partial \imag(f(\vz))}{\partial \real(\vz)}
\end{equation}
By definition,
\begin{equation}
\frac{\partial g(f(\vz))}{\partial \real(f(\vz))} = \real\left(\left(\nabla_{f(\vz)} g(f(\vz))\right)^H\right)
\end{equation}
\begin{equation}
\frac{\partial g(f(\vz))}{\partial \imag(f(\vz))} = -\imag\left(\left(\nabla_{f(\vz)} g(f(\vz))\right)^H\right)
\end{equation}
Using the fact that $\frac{\partial f(\vz)}{\partial \vz} = \lim_{\Delta \vz \rightarrow \vzero} \frac{f(\vz + \Delta \vz) - f(z)}{\Delta \vz}$,
\begin{equation}
\frac{\partial \real(f(\vz))}{\partial \real(\vz)} = \real\left(\frac{\partial f(\vz)}{\partial \vz}\right)
\end{equation}
\begin{equation}
\frac{\partial \imag(f(\vz))}{\partial \real(\vz)} = \imag\left(\frac{\partial f(\vz)}{\partial \vz}\right)
\end{equation}
Therefore,
\begin{multline}
\frac{\partial g(f(\vz))}{\partial \real(\vz)} = \real\left(\left(\nabla_{f(\vz)} g(f(\vz))\right)^H\right)\real\left(\frac{\partial f(\vz)}{\partial \vz}\right) \\- \imag\left(\left(\nabla_{f(\vz)} g(f(\vz))\right)^H\right)\imag\left(\frac{\partial f(\vz)}{\partial \vz}\right)
\end{multline}
\begin{equation} \label{equation:Partial Derivative for Real Component}
\frac{\partial g(f(\vz))}{\partial \real(\vz)} = \real\left(\left(\nabla_{f(\vz)} g(f(\vz))\right)^H\frac{\partial f(\vz)}{\partial \vz}\right)
\end{equation}

Having computed the partial derivative in respect to the real component, it is necessary to now compute the partial derivative in respect to the imaginary component. Following the same process using the chain rule,

\begin{equation}
\frac{\partial g(f(\vz))}{\partial \imag(\vz)} = \frac{\partial g(f(\vz))}{\partial \real(f(\vz))}\frac{\partial \real(f(\vz))}{\partial \imag(\vz)} + \frac{\partial g(f(\vz))}{\partial \imag(f(\vz))}\frac{\partial \imag(f(\vz))}{\partial \imag(\vz)}
\end{equation}
Using the fact that $\frac{\partial f(\vz)}{\partial \vz} = \lim_{\Delta \vz \rightarrow \vzero} \frac{f(\vz + \Delta \vz) - f(z)}{\Delta \vz}$,
\begin{equation}
\frac{\partial \real(f(\vz))}{\partial \imag(\vz)} = -\imag\left(\frac{\partial f(\vz)}{\partial \vz}\right)
\end{equation}
\begin{equation}
\frac{\partial \imag(f(\vz))}{\partial \imag(\vz)} = \real\left(\frac{\partial f(\vz)}{\partial \vz}\right)
\end{equation}
Therefore,
\begin{multline}\label{equation:Partial Derivative for Imaginary Component}
\frac{\partial g(f(\vz))}{\partial \imag(\vz)} = -\real\left(\left(\nabla_{f(\vz)} g(f(\vz))\right)^H\right)\imag\left(\frac{\partial f(\vz)}{\partial \vz}\right) \\- \imag\left(\left(\nabla_{f(\vz)} g(f(\vz))\right)^H\right)\real\left(\frac{\partial f(\vz)}{\partial \vz}\right)
\end{multline}

Finally, combining equations \ref{equation:Partial Derivative for Real Component} and \ref{equation:Partial Derivative for Imaginary Component} demonstrates that the computations using the gradient-based chain rule satisfy the definition in equation \ref{equation:Gradient Definition}.
\begin{equation}
\nabla_{\vz} g(f(\vz)) = \left(\frac{\partial g(f(\vz))}{\partial \real(\vz)} - j\frac{\partial g(f(\vz))}{\partial \imag(\vz)}\right)^H
\end{equation}

\section{Partial Derivatives}

\subsection{Partial Derivatives in Respect to Inputs}
The function $f_{\hat{\mD}}(\vb)$ is analytic in respect to $\vb$, and its partial derivative in respect to $\vb$ requires no explanation:
\begin{equation}
\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \vb} = \left(\rho\mId + \hat{\mD}^H\hat{\mD}\right)^{-1}
\end{equation}

Using the Woodbury matrix identity, equation \ref{equation:Standard Inversion Function} can be rewritten as
\begin{equation}
f_{\hat{\mD}}(\vb) = \frac{1}{\rho}\left(\vb - \hat{\mD}^H\left(\rho\mId + \hat{\mD}\hat{\mD}^H\right)^{-1}\hat{\mD}\vb\right) \label{equation:Woodbury Inversion Function}
\end{equation}
This form contains the function
\begin{equation}
g_{\hat{\mD}}(\vy) = \left(\rho\mId + \hat{\mD}\hat{\mD}^H\right)^{-1}\vy
\end{equation}
%
The partial derivative of $g_{\hat{\mD}}(\vy)$ in respect to $\vy$ is also straightforward, given its analytic nature:
\begin{equation}
\frac{\partial g_{\hat{\mD}}(\vy)}{\partial \vy} = \left(\rho\mId + \hat{\mD}\hat{\mD}^H\right)^{-1}
\end{equation}

The function $g_{\hat{\mD}}$ from the Woodbury form uses a different inverse: $\left(\rho\mId + \hat{\mD}\hat{\mD}^H\right)^{-1}$. However, the derivations for its derivatives are very similar to that of the previous inverse $\left(\rho\mId + \hat{\mD}^H\hat{\mD}\right)^{-1}$, so the focus here will remain on the original formulation without the Woodbury transformation.

\subsection{Partial Derivatives in Respect to Dictionary} \label{section:Differentiating Standard Form}
Let
%
\begin{equation}
\mQ = \rho\mId + \hat{\mD}^H\hat{\mD}
\end{equation}
%
so $f_{\hat{\mD}}(\vb) = \mQ^{-1}\vb$.

According to \cite{petersen2008matrix},
\begin{equation} \label{equation:Derivative of Inverse}
\frac{\partial (\rho\mId + \hat{\mD}^H\hat{\mD})^{-1}}{\partial a} = -\mQ^{-1}\frac{\partial \mQ}{\partial a}\mQ^{-1}
\end{equation}
where $a$ is a real scalar variable.

\begin{equation}
\frac{\partial \mQ}{\partial a} = \frac{\partial \rho\mId}{\partial a} + \frac{\partial \hat{\mD}^H\hat{\mD}}{\partial a} 
\end{equation}
\begin{equation}
\left(\hat{\mD}^H\hat{\mD}\right)_{m_1,m_2} = \sum_i \hat{D}_{i,m_2}\hat{D}_{i,m_1}^*
\end{equation}
where $\hat{D}_{i,m_1}^*$ is the complex conjugate of $\hat{D}_{i,m_1}$.
For, $m_1 \neq m_2$:
\begin{equation} \label{equation:Element Derivatives}
\begin{split}
\frac{\partial \left(\hat{\mD}^H\hat{\mD}\right)_{m_1,m_2}}{\partial 
\real(\hat{D}_{i,m_1})} = \hat{D}_{i,m_2} 
\\
\frac{\partial \left(\hat{\mD}^H\hat{\mD}\right)_{m_1,m_2}}{\partial \imag(\hat{D}_{i,m_1})} = -j\hat{D}_{i,m_2} 
\\
\frac{\partial \left(\hat{\mD}^H\hat{\mD}\right)_{m_1,m_2}}{\partial 
\real(\hat{D}_{i,m_2})} = \hat{D}_{i,m_1}^* 
\\
\frac{\partial \left(\hat{\mD}^H\hat{\mD}\right)_{m_1,m_2}}{\partial \imag(\hat{D}_{i,m_2})} = j\hat{D}_{i,m_1}^* 
\end{split}
\end{equation}
For the case $m_1 = m_2$:
\begin{equation}
\begin{split} \label{equation:Element Derivatives for Diagonal}
\frac{\partial \left(\hat{\mD}^H\hat{\mD}\right)_{m_1,m_1}}{\partial 
\real(\hat{D}_{i,m_1})} = 2
\real(\hat{D}_{i,m_1})
\\
\frac{\partial \left(\hat{\mD}^H\hat{\mD}\right)_{m_1,m_1}}{\partial \imag(\hat{D}_{i,m_1})} = 2\imag(\hat{D}_{i,m_1})
\end{split}
\end{equation}
Combining equations \ref{equation:Element Derivatives} and \ref{equation:Element Derivatives for Diagonal} yields the following equations:
\begin{equation}
\begin{split}
\frac{\partial \left(\hat{\mD}^H\hat{\mD}\right)}{\partial 
\real(\hat{D}_{c,m})} = \ve_m\ve_c^T\hat{\mD} + \hat{\mD}^H\ve_c\ve_m^T
\\
\frac{\partial \left(\hat{\mD}^H\hat{\mD}\right)}{\partial \imag(\hat{D}_{c,m})} = -j\ve_m\ve_c^T\hat{\mD} + j\hat{\mD}^H\ve_c\ve_m^T
\end{split}
\end{equation}
where $\ve_i$ is the $i$th Euclidean basis vector.\footnote{That is, the $i$th element of vector $\ve_i$ is equal to one, and all other elements are zero.}
\begin{equation}
\frac{\partial \rho\mId}{\partial \hat{D}_{c,m}} = 0
\end{equation}
So,
\begin{equation}
\begin{split}
\frac{\partial \mQ}{\partial 
\real(\hat{D}_{c,m})} = \ve_m\ve_c^T\hat{\mD} + \hat{\mD}^H\ve_c\ve_m^T
\\
\frac{\partial \mQ}{\partial \imag(\hat{D}_{c,m})} = -j\ve_m\ve_c^T\hat{\mD} + j\hat{\mD}^H\ve_c\ve_m^T
\end{split}
\end{equation}

Finally, this result can be plugged back into equation \ref{equation:Derivative of Inverse}
\begin{equation}
\begin{split}
\frac{\partial (\rho\mId + \hat{\mD}^H\hat{\mD})^{-1}}{\partial 
\real(\hat{D}_{c,m})} = -\mQ^{-1}(\ve_m\ve_c^T\hat{\mD} + \hat{\mD}^H\ve_c\ve_m^T )\mQ^{-1}
\\
\frac{\partial (\rho\mId + \hat{\mD}^H\hat{\mD})^{-1}}{\partial \imag(\hat{D}_{c,m})} = \mQ^{-1}(j\ve_m\ve_c^T\hat{\mD} - j\hat{\mD}^H\ve_c\ve_m^T)\mQ^{-1}
\end{split}
\end{equation}
\begin{equation}\label{equation:Partial Dictionary Derivative}
\begin{split}
\frac{\partial f_{\hat{\mD}}(\vb)}{\partial 
\real(\hat{D}_{c,m})} = \mQ^{-1}(-\ve_m\ve_c^T\hat{\mD} - \hat{\mD}^H\ve_c\ve_m^T )\mQ^{-1}\vb
\\
\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \imag(\hat{D}_{c,m})} = \mQ^{-1}(j\ve_m\ve_c^T\hat{\mD} - j\hat{\mD}^H\ve_c\ve_m^T)\mQ^{-1}\vb
\end{split}
\end{equation}



The derivations for the partial derivatives of
\begin{equation}
g_{\hat{\mD}}(\vy) = (\rho\mId + \mD\mD^H)^{-1}\vy
\end{equation}
are very similar to that of $f_{\hat{\mD}}$. Let $\mQ_g = \rho\mId + \mD\mD^H$. Given the similarity between the derivations, the calculations will not be repeated here, though through the same procedure as before,
%
\begin{equation}
\begin{split}
\frac{\partial \mQ_g}{\partial 
\real(\hat{D}_{c,m})} = \ve_c\ve_m^T\hat{\mD}^H + \hat{\mD}\ve_m\ve_c^T
\\
\frac{\partial \mQ_g}{\partial \imag(\hat{D}_{c,m})} = j\ve_c\ve_m^T\hat{\mD}^H - j\hat{\mD}\ve_m\ve_c^T
\end{split}
\end{equation}
%
\begin{equation}
\begin{split}
\frac{\partial (\rho\mId + \hat{\mD}\hat{\mD}^H)^{-1}}{\partial 
\real(\hat{D}_{c,m})} = -\mQ_g^{-1}(\ve_c\ve_m^T\hat{\mD}^H + \hat{\mD}\ve_m\ve_c^T)\mQ_g^{-1}
\\
\frac{\partial (\rho\mId + \hat{\mD}\hat{\mD}^H)^{-1}}{\partial \imag(\hat{D}_{c,m})} = -\mQ_g^{-1}(j\ve_c\ve_m^T\hat{\mD}^H - j\hat{\mD}\ve_m\ve_c^T)\mQ_g^{-1}
\end{split}
\end{equation}

\section{Backpropagation of Loss Function}
To backpropagate the loss function $\loss$ through component function $f_{\hat{\mD}}(\vb)$, the partial derivative in respect to the input of function $f_{\hat{\mD}}(\vb)$ is necessary to reach earlier layers, as explained in section \ref{section:Chain Rule}. If $\vb$ only influenced $\loss$ through $f_{\hat{\mD}}$, then
\begin{equation}
\nabla_{\vb} \loss = \left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \vb}
\end{equation}

If the dictionary $\hat{\mD}$ is only used in component function $f_{\hat{\mD}}$, then\footnote{These equations can be derived through chain rule. Note that the function of $\hat{\mD}_{c,m}$ is non-analytic, so to apply chain rule properly, it is important to work with the real and imaginary components, rather than directly with complex numbers.}
\begin{equation}
\frac{\partial \loss}{\partial \real(\hat{D}_{c,m})} = \real\left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \real(\hat{D}_{c,m})}\right)
\end{equation}
\begin{equation}
\frac{\partial \loss}{\partial \imag(\hat{D}_{c,m})} = \real\left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \imag(\hat{D}_{c,m})}\right)
\end{equation}

\sloppy If the dictionary $\hat{\mD}$ is used in multiple component functions, the expressions $\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \real(\hat{D}_{c,m})}$ and $\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \imag(\hat{D}_{c,m})}$ still must be calculated so that the results can be aggregated later. Similarly, if $\vb$ affects $\loss$ through multiple branches,\footnote{That is, not just through function $f_{\hat{\mD}}$} the expression $\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \vb}$ still must be calculated, and $\nabla_{\vb} \loss$ is just an aggregation of the results across all branches.

\begin{equation}
\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \vb^H} =\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}
\end{equation}

\begin{equation}
\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \real(\hat{D}_{c,m})} = \left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}(-\ve_m\ve_c^T\hat{\mD} - \hat{\mD}^H\ve_c\ve_m^T)\mQ^{-1}\vb
\end{equation}

\begin{multline}
\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \real(\hat{D}_{c,m})} = -\left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\ve_m\right)(\ve_c^T\hat{\mD}\mQ^{-1}\vb) \\- \left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\hat{\mD}^H\ve_c\right)(\ve_m^T\mQ^{-1}\vb)
\end{multline}

Note that $\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\hat{\mD}^H\ve_c$ and $\ve_m^T\mQ^{-1}\vb$ are scalars and $\mQ^{-1}$ is Hermitian, so using the fact that the Hermitian transpose of a scalar is its complex conjugate
\begin{equation}
\begin{split}
\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\hat{\mD}^H\ve_c = & \left(\ve_c^T\hat{\mD}\mQ^{-1}\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^*
\\
\ve_m^T\mQ^{-1}\vb = & (\vb^H\mQ^{-1}\ve_m)^*
\end{split}
\end{equation}
Therefore,
\begin{multline}
\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \real(\hat{D}_{c,m})} = -\left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\ve_m\right)(\ve_c^T\hat{\mD}\mQ^{-1}\vb) \\- \left(\ve_c^T\hat{\mD}\mQ^{-1}\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^* (\vb^H\mQ^{-1}\ve_m)^*
\end{multline}
Scalar multiplication commutes, so
\begin{multline}
\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \real(\hat{D}_{c,m})} = -(\ve_c^T\hat{\mD}\mQ^{-1}\vb)\left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\ve_m\right) \\- \left(\ve_c^T\hat{\mD}\mQ^{-1}\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^* (\vb^H\mQ^{-1}\ve_m)^*
\end{multline}
\begin{multline}
\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \real(\hat{D}_{c,m})} = \ve_c^T(-(\hat{\mD}\mQ^{-1}\vb)\left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\right) \\- \left(\hat{\mD}\mQ^{-1}\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^* (\vb^H\mQ^{-1})^*)\ve_m
\end{multline}
\begin{equation}
\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \real(\hat{D}_{c,m})} = \left(-(\hat{\mD}\mQ^{-1}\vb)\left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\right) - \left(\hat{\mD}\mQ^{-1}\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^* (\vb^H\mQ^{-1})^*\right)_{c,m}
\end{equation}

\begin{equation}
\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \imag(\hat{D}_{c,m})} = \left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}(j\ve_m\ve_c^T\hat{\mD} - j\hat{\mD}^H\ve_c\ve_m^T)\mQ^{-1}\vb
\end{equation}

\begin{multline}
\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \imag(\hat{D}_{c,m})} = j\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\ve_m\ve_c^T\hat{\mD}\mQ^{-1}\vb \\- j\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\hat{\mD}^H\ve_c\ve_m^T\mQ^{-1}\vb
\end{multline}

\begin{multline}
\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \imag(\hat{D}_{c,m})} = j\left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\ve_m\right)(\ve_c^T\hat{\mD}\mQ^{-1}\vb) \\- j\left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\hat{\mD}^H\ve_c\right)(\ve_m^T\mQ^{-1}\vb)
\end{multline}

\begin{multline}
\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \imag(\hat{D}_{c,m})} = j(\ve_c^T\hat{\mD}\mQ^{-1}\vb)\left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\ve_m\right) \\- j\left(\ve_c^T\mD\mQ^{-1}\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^*(\vb^H\mQ^{-1}\ve_m)^*
\end{multline}


\begin{equation}
\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \imag(\hat{D}_{c,m})} = j\left(-\left(\mD\mQ^{-1}\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^*(\vb^H\mQ^{-1})^* + (\hat{\mD}\mQ^{-1}\vb)\left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\right)\right)_{c,m}
\end{equation}



Assuming that $\hat{D}_{c,m}$ only affects $\loss$ through $f_{\mD}(\vb)$,
\begin{equation}
\frac{\partial \loss}{\partial \real(\hat{D}_{c,m})} = \real\left(-(\hat{\mD}\mQ^{-1}\vb)\left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\right) - \left(\hat{\mD}\mQ^{-1}\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^* (\vb^H\mQ^{-1})^*\right)_{c,m}
\end{equation}
\begin{equation}
\frac{\partial \loss}{\partial \imag(\hat{D}_{c,m})} = \real\left(j\left(-\mD\mQ^{-1}\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^*(\vb^H\mQ^{-1})^* + j(\hat{\mD}\mQ^{-1}\vb)\left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\right)\right)_{c,m}
\end{equation}
\begin{equation}
\frac{\partial \loss}{\partial \imag(\hat{D}_{c,m})} = \imag\left(-\left(\mD\mQ^{-1}\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)(\vb^H\mQ^{-1}) - (\hat{\mD}\mQ^{-1}\vb)\left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\right)\right)_{c,m}
\end{equation}
Therefore,
\begin{equation}
\frac{\partial \loss}{\partial \hat{D}_{c,m}} = \left(-(\hat{\mD}\mQ^{-1}\vb)^*\left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\right)^* - \left(\hat{\mD}\mQ^{-1}\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^* (\vb^H\mQ^{-1})^*\right)_{c,m}
\end{equation}
\begin{equation}
\nabla_{\hat{\mD}} \loss = -(\hat{\mD}\mQ^{-1}\vb)\left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\right) - \left(\hat{\mD}\mQ^{-1}\nabla_{f_{\hat{\mD}}(\vb)} \loss\right) (\vb^H\mQ^{-1})
\end{equation}

If $\hat{\mD}$ affects $\loss$ through other functions (rather than just through $f_{\mD}(\vb)$),
\begin{equation}
\nabla^{(\vb \rightarrow f_{\mD}(\vb))}_{\hat{\mD}} \loss = -(\hat{\mD}\mQ^{-1}\vb)\left(\left(\nabla_{f_{\hat{\mD}}(\vb)} \loss\right)^H\mQ^{-1}\right) - \left(\hat{\mD}\mQ^{-1}\nabla_{f_{\hat{\mD}}(\vb)} \loss\right) (\vb^H\mQ^{-1})
\end{equation}


Using similar derivations for the function $g_{\hat{\mD}}(\vy)$ yields the following equation for the gradient term:
\begin{equation}
\nabla^{(\vy \rightarrow g_{\mD}(\vy))}_{\hat{\mD}} \loss = -\left(\mQ_g^{-1}\nabla_{g_{\hat{\mD}}(\vy)} \loss\right)(\vy^H\mQ_g^{-1}\hat{\mD}) - (\mQ_g^{-1}\vy)\left(\left(\nabla_{g_{\hat{\mD}}(\vy)} \loss\right)^H\mQ_g^{-1}\hat{\mD}\right)
\end{equation}


%\subsection{Woodbury Form}
%\begin{equation}
%\frac{\partial \loss}{\partial f_{\hat{\mD}}(\vb)}\frac{\partial f_{\hat{\mD}}(\vb)}{\partial \vb^H} = \frac{1}{\rho}(\frac{\partial \loss}{\partial f_{\hat{\mD}}(\vb)} - \frac{\partial \loss}{\partial f_{\hat{\mD}}(\vb)}\mD^H\mQ^{-1}\mD)
%\end{equation}

\end{appendices}
