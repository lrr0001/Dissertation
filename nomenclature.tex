\clearpage
\begin{centering}
\textbf{NOMENCLATURE}\\
\vspace{\baselineskip}
\end{centering}
\begin{singlespace}
\section*{Acronyms}

\begin{tabular}{cp{0.8\textwidth}}
ADMM & the Alternating Directions Method of Multipliers algorithm\np
ISTA & the Iterative Shrinkage Thresholding Algorithm \np
FISTA & the Fast Iterative Shrinkage Tresholding Algorithm \np
RGB & Describes an image with red, green, and blue channels \np
JPEG & an image compression standard developed by the Joint Photographic Experts Group \np
FFT & the discrete Fourier transform \np
DCT & type-II discrete cosine transform \np
CPU & central processing unit \np
RAM & random-access memory
\end{tabular}

\section*{Integer Constants}
Capital letters that are not bold are used to denote integer constants.\np
\begin{tabular}{cp{0.8\textwidth}}
$M$ & the number of filters\np
$C$ & the number of channels\np
$\hat{K}$ & the number of elements in a single channel of the signal\np
$K$ & the number of elements in a single channel of a filter \np
$\reflectbox{R}$ & a small integer that specifies the rank of the dictionary updates \np
$L$ & the number of layers
\end{tabular}

\section*{Matrices}
Bold capital letters are used to denote matrices.\np
\begin{tabular}{cp{0.8\textwidth}}
$\mD$ & the dictionary. For most of the dissertation, $\mD$ has circulant matrix blocks. \np
%
$\mG$ & another variable used to represent the dictionary. $\mG$ is used when it is necessary to distinguish $2$ different dictionaries, like when dictionary $\mD$ is used to approximate dictionary $\mG$. \np
%
$\mS$ & a collection of signal vectors, either gathering multiple channels or multiple samples. \np
%
$\mX$ & a collection of dictionary coefficient vectors, corresponding to multple signal vectors. \np
%
$\mA$, $\mB$  & the linear operators appearing the the affine constraints for ADMM. $\mA$ and $\mB$ are also used as arbitary matrices when discussing general linear algebra principles. \np
%
$\mU$, $\mV$ & arbitrary matrices used for discussing general linear algebra principles and low-rank updates. \np
%
$\mPhi$ & an arbitrary linear operator \np
%
$\mT$ & a diagonal matrix that has diagonal elements of $1$ for dictionary elements that are not constrained to be zero, and zeros for the other diagonal elements \np
%
$\mW$ & a matrix which converts an image vector from RGB to YUV, downsamples the UV channels, and computes the DCT  of $8 \times 8$ blocks. \np
%
$\mSS$ & part of the dictionary for the product dictionary model \np
%
$\mQ$ & used to denote the matrix $\rho\mId + \hat{\mD}^H\hat{\mD}$ \np
%
$\mXi$ & used to denote the matrix $\rho\mId + \hat{\mD}\hat{\mD}^H$ \np
%
$\mR$ & a rescaling matrix, used to scale a normalized dictionary back to its unnormalized form
\end{tabular}

\section*{Vectors}
Bold, lower-case letters are used to denote vectors.\np
\begin{tabular}{cp{0.8\textwidth}}
$\vx$, $\vy$ & the primal variables for ADMM. \np
%
$\vu$ & the dual variable for ADMM. \np
%
$\vc$ & the constraint vector in ADMM. \np
%
$\vx$, $\vz$ & the coefficients for dictionary model in ADMM algorithm. \np
%
$\vv$ & another primal variable (grouped with $\vz$) used in chapter $4$. \np
%
$\vgamma$ & the corresponding dual variable. \np
%
$\vx$, $\vz$ & also used as vectors in the FISTA algorithm. \np
%
$\vx$ & also used as an arbitrary vector throughout document.  Context will make clear. \np
%
$\vs$ & the signal. \np
%
$\vb$ & another arbitrary vector. \np
%
$\vu$, $\vv$ & more arbitrary vectors, usually used in pairs. The may collectively specify a rank-$1$ update to a matrix: $\vu\vv^H$. \np
%
$\vf$ & a dictionary filter. \np
%
$\vd$ & a column of $\mD$. \np
%
$\vq$ & the quality-factor dependent vector used in quantization in JPEG compression. \np
%
$\vomega$ & an eigenvector.
\end{tabular}



\section*{Non-Integer Scalars}
Lower-case letters that are not bold are generally used to denote scalars that are not integer constants. $\mathbb{L}$ and $\loss$ are the exceptions.\np
\begin{tabular}{cp{0.8\textwidth}}
$a$, $b$ & arbitrary scalars. \np
%
$\rho$ & a scalar for the ADMM alrogithm that specifies both the weighting of the constraints in the augmented Lagrangian and the stepsize in the dual variable update. \np
%
$\alpha$ & the over-relaxation or under-relaxation factor for ADMM \np
%
$\lambda$ & the factor for the $\operatorname{L}_1$ penalty.  $\lambda$ is also used as the factor for the $\operatorname{L}_2$ penalty on the image gradients for Tikhonov regularization. Context will make clear. \np
%
$\mu$ & the factor for the $\operatorname{L}_2$ penalty for the reconstruction of the signal or coefficients from the previous layer. \np
%
$\mathbb{L}$ & an estimate of the Lipshitz constant, used to determine stepsize in the ISTA and FISTA algorithms. \np
%
$\tau$ & the eigenvalue \np
%
$\eta_{\vu,\vv}$ & used to represent $\vu^H\hat{\mD}\vv$ \np
%
$\eta_{\vu}$ & used to represent $\|\mD^H\vu\|_2^2$ \np
%
$\eta_{\vv}$ & used to represent $\|\vv\|_2^2$ \np
%
$r$, $\omega$ & used to specify momentum stepsize in FISTA and a FISTA-like algorithm. Both always appear with superscripts specifying iteration. \np
%
$\loss$ & loss (as in the loss function)
\end{tabular}

\section*{Functions and Operations}
\begin{tabular}{cp{0.8\textwidth}}
$*$ & Circular convolution \np
%
$a^{*}$ & the complex conjugate of $a$. \np
%
$\vu^H$, $\mU^H$ & the Hermitian transpose of a vector $\vu$ or matrix $\mU$ \np
%
$\L_{\rho}$ & the augmented Lagrangian function \np
%
$\operatorname{S}$ & the shrinkage operator \np
%
$\F$ & applies the Fourier tranform to each channel and/or filter \np
%
$f$ and $g$ & arbitrary convex functions. $f$ may also be used to specify the objective function of a minimization problem. $f$ and $g$ are also used for arbitrary functions that are part of a composite function. Context should make clear. \np
%
$\operatorname{q}(\cdot)$ & quantizes a vector \np
%
$\indicator_{\text{condition}}$ & takes on a value of $0$ when the condition is true and $\infty$ when the condition is false. \np
%
$\arg \min$ & the argument minimum of an expression \np
%
$\nabla_{a} b$ & the gradient of $b$ in respect to $a$ \np
%
$L_1$,$\|\cdot\|_1$ & the $L_1$ norm \np
%
$L_2$,$\|\cdot\|_2$ & the $L_2$ norm \np
%
$\mPhi$ & n arbitrary linear operator (this operator is also listed under matrices, since matrices are linear operators) \np
%
$\mT$ & zeros out all dictionary elements that are constrained to be zero. (This operator is also listed under matrices since matrices are linear operators.) \np
%
$\mW$ & converts from RGB to YUV, downsamples the UV channels, and computes the DCT  of $8 \times 8$ blocks. (This operator also appears under matrices.) \np
%
$\real$ & selects the real component of a complex number, vector, or matrix \np
%
$\imag$ & selects the imaginary component of a complex number, vector, or matrix. (Output is real.)
\end{tabular}
\section*{Superscripts}
\begin{tabular}{cp{0.8\textwidth}}
$a^b$ & An exponent: the $b^{\text{th}}$ power of $a$ \np
%
$a^*$ & the complex conjugate of $a$ \np
%
$\vu^H$, $\mU^H$ & the Hermitian transpose of vector $\vu$ or matrix $\mU$ \np
%
$i^{\text{th}}$ & occuring at position $i$ in a sequence \np
%
$\vgamma^{(t)}$ & the value of vector $\vgamma$ at the $t^{\text{th}}$ iteration \np
%
$\mD^{(n)}$ & the value of dictionary $\mD$ at the $n^{\text{th}}$ dictionary update\np
%
$\nabla_a^{(\vx \rightarrow \vy)} \loss$ & the gradient term of $\loss$ in respect to $a$ that corresponds to the computation of $\vy$ from $\vx$. The gradient of $\loss$ in respect to $a$ is the sum of all the gradient terms.
\end{tabular}

\section*{Subscripts}
\begin{tabular}{cp{0.8\textwidth}}
$m$ & specifies the filter. If there are multiple layers, $[m]$ will be used instead of subscript $m$. \np
%
$n$ & specifies the sample. \np
%
$c$ & specifies the channel. If there are multiple layers $[c]$ will be used instead of subscript $c$. \np
%
$\ell$ & specifies the layer. \np
%
$+$ and $-$ & used to specify the eigenvalues or eigenvectors of a $2 \times 2$ matrix corresponding to the plus or minus in the quadratic formula. \np
%
$\rho$ in $\L_{\rho}$ & specifies the scalar weight of the $L_2$ norm related to the affine constraints, used in the augmented Lagrangian function. \np
%
init in $\mD_{\text{init}}$ & short for initial value, appears in algorithms. \np
%
sc in $\vgamma_{\text{sc}}$ & short for "scaled", and indicates that the variable in the algorithm is a scaled form of the variable. \np
%
$i$ & used for essentially all other subscript indexing.
\end{tabular}

\section*{Indexing Integers}
Lower case letters that are not bold are also used to denote indexing integers.\np
\begin{tabular}{cp{0.8\textwidth}}
$n$ & selects the dictionary update or corresponding signal \np
%
$t$ & selects the iteration \np
%
$m$ & selects the filter \np
%
$c$ & selects the channel \np
%
$\hat{k}$ & specifies the frequency \np

$i$ & used for most other indexing.
\end{tabular}



\end{singlespace}
\clearpage

