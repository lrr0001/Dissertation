\chapter{Conclusion}
This dissertation has presented a novel dictionary learning algorithm for signals with a large number of channels and its application to multi-layer dictionary models. This algorithm can be used for JPEG artifact removal, and shows some promise as a competitor to the FISTA algorithm, given its faster convergence in solving the sparse-coding problem, but does have larger memory requirements than FISTA. In addition, appendix A generalizes the work of \cite{krause2015more} to handle complex numbers, which to my knowledge has not been done previously. \cite{krause2015more} efficiently computes rank-$1$ updates for a Cholesky factorization.

In addition to seeking other applications, future expansion on this research might look to adapt $\rho$ during training. While $\rho$ must remain fixed for efficient updates of the Cholesky decomposition (or LDLT decomposition), drift over many iterations eventually must be rectified by recomputing the entire decomposition. At such time, an update to $\rho$ would require no additional computational cost. While many methods exist to adapt $\rho$ for ADMM \cite{he2000alternating}\cite{xu2017adaptive}, these methods are designed to adjust $\rho$ far more frequently. Another potential area of future research lies in tailoring momentum methods for dictionary updates to this novel dictionary learning algorithm. While the dictionary learning algorithm is not incompatible with momentum-based dictionary updates, conventional momentum methods \cite{sutskever2013importance}\cite{kingma2017adam} will not take into account the low-rank approximation step, which may hurt performance. Finally, since the memory requirements promote the use of frames to decrease the signal size, there may be a need in some applications to combine results across frames. Fortunately, given the popularity of frame-based dictionary models, there is an existing body of work to build off of for this task \cite{elad2006image}\cite{yang2010image}\cite{turquais2017method}\cite{jiang2021combining}.
